{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = [14,30,60,90,180]\n",
    "num_features = 56 #depends on how much features we have\n",
    "output_days = 7\n",
    "output_features = 2 #min and max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-weighted loss function definition\n",
    "# Custom Time-Weighted Loss Function\n",
    "# Custom Keras Loss Class\n",
    "class TimeWeightedLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, weights, lambda_=1.0, gamma=0.1, name=\"time_weighted_loss\"):\n",
    "        super().__init__(name=name)\n",
    "        self.weights = tf.convert_to_tensor(weights, dtype=tf.float32)\n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Split y_true and y_pred into min and max\n",
    "        y_true_min, y_true_max = y_true[:, :, 1], y_true[:, :, 0]\n",
    "        y_pred_min, y_pred_max = y_pred[:, :, 1], y_pred[:, :, 0]\n",
    "\n",
    "        # Ensure weights are properly shaped\n",
    "        weights = tf.reshape(self.weights, [1, 7])  # Shape for broadcasting\n",
    "\n",
    "        # Time-weighted MSE for min and max\n",
    "        mse_min = tf.reduce_mean(weights * tf.square(y_true_min - y_pred_min))\n",
    "        mse_max = tf.reduce_mean(weights * tf.square(y_true_max - y_pred_max))\n",
    "\n",
    "        # Time-weighted min-max constraint penalty\n",
    "        penalty = tf.reduce_sum(weights * tf.square(tf.maximum(0.0, y_pred_min - y_pred_max)))\n",
    "\n",
    "        # Time-weighted temporal smoothness\n",
    "        smoothness_min = tf.reduce_sum(weights[:, 1:] * tf.square(y_pred_min[:, 1:] - y_pred_min[:, :-1]))\n",
    "        smoothness_max = tf.reduce_sum(weights[:, 1:] * tf.square(y_pred_max[:, 1:] - y_pred_max[:, :-1]))\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = (mse_min + mse_max) + self.lambda_ * penalty + self.gamma * (smoothness_min + smoothness_max)\n",
    "        return total_loss\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"weights\": self.weights.numpy().tolist(), \"lambda_\": self.lambda_, \"gamma\": self.gamma}\n",
    "\n",
    "# Define weights (e.g., exponential decay)\n",
    "alpha = 0.7\n",
    "weights = [alpha ** (d - 1) for d in range(1, 8)]\n",
    "\n",
    "# Instantiate the custom loss\n",
    "custom_loss = TimeWeightedLoss(weights=weights, lambda_=1.0, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_LSTM_model(window_size, num_features, output_days, output_features):\n",
    "    inputs = layers.Input(shape=(window_size, num_features))\n",
    "    \n",
    "    x = layers.LSTM(100, return_sequences=True)(inputs)\n",
    "    x = layers.LSTM(100)(x)\n",
    "    \n",
    "    x = layers.Dense(output_days * output_features)(x)  # Output for 7 days * 2 features (min and max)\n",
    "    \n",
    "    x = layers.Reshape((output_days, output_features))(x)\n",
    "    \n",
    "    model = models.Model(inputs, x)\n",
    "    function_name = baseline_LSTM_model.__name__\n",
    "    \n",
    "    return model, function_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_GRU_model(window_size, num_features, output_days, output_features):\n",
    "    inputs = layers.Input(shape=(window_size, num_features))\n",
    "    \n",
    "    x = layers.GRU(100, return_sequences=False)(inputs)\n",
    "    \n",
    "    x = layers.Dense(output_days * output_features)(x)  # Output for 7 days * 2 features (min and max)\n",
    "    \n",
    "    x = layers.Reshape((output_days, output_features))(x)\n",
    "    \n",
    "    model = models.Model(inputs, x)\n",
    "    function_name = baseline_GRU_model.__name__\n",
    "    \n",
    "    return model, function_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CNN_LSTM mdoel is a combination of CNN and LSTM. CNN is used to extract the features from the input data and LSTM is used to support the sequence data.\n",
    "'''\n",
    "def CNN_LSTM_model(window_size, num_features, output_days, output_features):\n",
    "    inputs = layers.Input(shape=(window_size, num_features))\n",
    "    \n",
    "    x = layers.Conv1D(filters=64, kernel_size=2, activation='relu')(inputs)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = layers.Conv1D(filters=128, kernel_size=2, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.LSTM(100, return_sequences=True)(x)\n",
    "    x = layers.LSTM(100)(x)\n",
    "    \n",
    "    x = layers.Dense(output_days * output_features)(x)  # 7 days * 2 targets (High, Low)\n",
    "    \n",
    "    x = layers.Reshape((output_days, output_features))(x)\n",
    "    \n",
    "    model = models.Model(inputs, x)\n",
    "    function_name = CNN_LSTM_model.__name__\n",
    "    \n",
    "    return model, function_name\n",
    "#can use KAN, Attenion layer to adjust the weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN + GRU model\n",
    "def CNN_GRU_model(window_size, num_features, output_days, output_features):\n",
    "    inputs = layers.Input(shape=(window_size, num_features))\n",
    "    \n",
    "    x = layers.Conv1D(filters=64, kernel_size=2, activation='relu')(inputs)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = layers.Conv1D(filters=128, kernel_size=2, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.GRU(100, return_sequences=False)(x)\n",
    "    \n",
    "    x = layers.Dense(output_days * output_features)(x)  # Output for 7 days * 2 features (min and max)\n",
    "    \n",
    "    x = layers.Reshape((output_days, output_features))(x)\n",
    "    \n",
    "    model = models.Model(inputs, x)\n",
    "    function_name = CNN_GRU_model.__name__\n",
    "    \n",
    "    return model, function_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Attention\n",
    "def CNN_LSTM_SA_model(window_size, num_features, output_days, output_features):\n",
    "    inputs = layers.Input(shape=(window_size, num_features))\n",
    "    x = layers.Conv1D(filters=64, kernel_size=2, activation='relu')(inputs)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = layers.Conv1D(filters=128, kernel_size=2, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = layers.LSTM(100, return_sequences=True)(x)\n",
    "    x = layers.LSTM(100, return_sequences=True)(x)\n",
    "    \n",
    "    # Attention mechanism\n",
    "    attention = Attention()([x, x])\n",
    "    x = layers.Concatenate()([x, attention])\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    x = layers.Dense(output_days*output_features)(x)\n",
    "    outputs = layers.Reshape((output_days, output_features))(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    function_name = CNN_LSTM_SA_model.__name__\n",
    "    return model, function_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_GRU_SA_model(window_size, num_features, output_days, output_features):\n",
    "    inputs = layers.Input(shape=(window_size, num_features))\n",
    "    \n",
    "    x = layers.Conv1D(filters=64, kernel_size=2, activation='relu')(inputs)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = layers.Conv1D(filters=128, kernel_size=2, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.GRU(100, return_sequences=True)(x)\n",
    "    \n",
    "    attention = layers.Attention()([x, x])  # Self-attention (query = value = x)\n",
    "    x = layers.Concatenate()([x, attention])\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(output_days * output_features)(x)\n",
    "    \n",
    "    outputs = layers.Reshape((output_days, output_features))(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    function_name = CNN_GRU_SA_model.__name__\n",
    "    \n",
    "    return model,function_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    \"\"\"Helper function to compute the angles for positional encoding.\"\"\"\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Computes the sinusoidal positional encoding.\n",
    "\n",
    "    Args:\n",
    "        seq_len (int): Length of the sequence.\n",
    "        d_model (int): Dimensionality of the model.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Positional encoding of shape (1, seq_len, d_model).\n",
    "    \"\"\"\n",
    "    pos = np.arange(seq_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rads = get_angles(pos, i, d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, d_model, num_heads, ff_dim, dropout_rate):\n",
    "    \"\"\"\n",
    "    Implements a single transformer encoder block.\n",
    "\n",
    "    Args:\n",
    "        inputs (tf.Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "        d_model (int): Dimensionality of the model.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        ff_dim (int): Dimensionality of the feed-forward network.\n",
    "        dropout_rate (float): Dropout rate.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Output tensor of the encoder block.\n",
    "    \"\"\"\n",
    "    # Multi-head self-attention\n",
    "    attn_output = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(inputs, inputs)\n",
    "    attn_output = tf.keras.layers.Dropout(dropout_rate)(attn_output)\n",
    "    out1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
    "    \n",
    "    # Feed-forward network\n",
    "    ffn_output = tf.keras.layers.Dense(ff_dim, activation='relu')(out1)\n",
    "    ffn_output = tf.keras.layers.Dense(d_model)(ffn_output)\n",
    "    ffn_output = tf.keras.layers.Dropout(dropout_rate)(ffn_output)\n",
    "    out2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFT_transformer_model(window_size, num_features, output_days, output_features,\n",
    "                            d_model=64, num_heads=4, ff_dim=128, num_transformer_blocks=2, dropout_rate=0.1):\n",
    "    \"\"\"\n",
    "    Builds a simplified Temporal Fusion Transformer (TFT)-inspired model for stock prediction.\n",
    "    \n",
    "    Args:\n",
    "        window_size (int): Number of time steps in the input sequence.\n",
    "        num_features (int): Number of features per time step.\n",
    "        output_days (int): Number of days (time steps) to predict.\n",
    "        output_features (int): Number of features to predict per day.\n",
    "        d_model (int, optional): Dimensionality of the model. Default is 64.\n",
    "        num_heads (int, optional): Number of attention heads in the transformer blocks. Default is 4.\n",
    "        ff_dim (int, optional): Dimensionality of the feed-forward network. Default is 128.\n",
    "        num_transformer_blocks (int, optional): Number of transformer encoder blocks. Default is 2.\n",
    "        dropout_rate (float, optional): Dropout rate. Default is 0.1.\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model: A compiled Keras model ready for training.\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=(window_size, num_features))\n",
    "    x = tf.keras.layers.Dense(d_model)(inputs)\n",
    "    pos_encoding = positional_encoding(window_size, d_model)\n",
    "    x = x + pos_encoding\n",
    "    \n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, d_model, num_heads, ff_dim, dropout_rate)\n",
    "    \n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = tf.keras.layers.Dense(output_days * output_features)(x)\n",
    "    outputs = tf.keras.layers.Reshape((output_days, output_features))(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    function_name = TFT_transformer_model.__name__\n",
    "    # Return the model along with a name for saving/identification\n",
    "    return model, function_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta Model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "def base_training(model_function,\n",
    "                loading_path, \n",
    "                saving_path, \n",
    "                version_name, \n",
    "                seed = 42,\n",
    "                test_size = 0.8,\n",
    "                epochs = 20,\n",
    "                batch_size = 8,\n",
    "                shuffle=True, \n",
    "                metrics=['mae'],\n",
    "                loss = 'mean_squared_error',\n",
    "                #loss = custom_loss,\n",
    "                optimizer='adam',\n",
    "                window_size = window_size, \n",
    "                num_features = num_features, \n",
    "                output_days = output_days, \n",
    "                output_features = output_features):\n",
    "    \n",
    "    model_history = []\n",
    "    models = []\n",
    "    X_tests = []\n",
    "    y_tests = []\n",
    "    #create folder to save the model\n",
    "    #if loss == custom_loss:\n",
    "    model_folder_path = os.path.join(saving_path, version_name, f\"_s-{seed}_t-{test_size}_e-{epochs}_b-{batch_size}_S-{shuffle}_m-{metrics}_l-custom_o-{optimizer}\")\n",
    "    #else:\n",
    "    #    model_folder_path = os.path.join(saving_path, version_name, f\"_s-{seed}_t-{test_size}_e-{epochs}_b-{batch_size}_S-{shuffle}_m-{metrics}_l-{loss}_o-{optimizer}\")\n",
    "\n",
    "    os.makedirs(model_folder_path, exist_ok=True)\n",
    "    print(f\"Folder '{version_name}' is ready.\")\n",
    "\n",
    "\n",
    "    for window in window_size:\n",
    "        # Load the data\n",
    "        X = pd.read_pickle(loading_path + f\"X_{window}days_i.pkl\")\n",
    "        y = pd.read_pickle(loading_path + f\"y_{window}days_i.pkl\")\n",
    "        print(f\"Data for {window} days loaded.\")\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                            y, \n",
    "                                                            test_size=test_size, \n",
    "                                                            shuffle=shuffle, \n",
    "                                                            random_state=seed)\n",
    "        print(\"Data split into training and testing sets.\")\n",
    "\n",
    "        # Train the model\n",
    "        print(\"start training:\\n\")\n",
    "        model, name = model_function(window, \n",
    "                                     num_features, \n",
    "                                     output_days, \n",
    "                                     output_features)\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "        \n",
    "        history = model.fit(X_train, \n",
    "                            y_train, \n",
    "                            epochs=epochs, \n",
    "                            batch_size=batch_size, \n",
    "                            validation_data=(X_test, y_test), \n",
    "                            verbose=1)\n",
    "        \n",
    "        # Save the model    \n",
    "        model_save_path = os.path.join(model_folder_path, f\"{name}_{window}days.h5\")\n",
    "        model.save(model_save_path)\n",
    "        print(f\"Model trained on {window} days has been saved.\")\n",
    "        models.append(model)\n",
    "        model_history.append(history)\n",
    "        X_tests.append(X_test)\n",
    "        y_tests.append(y_test)\n",
    "\n",
    "    return models, model_history, name, X_tests, y_tests\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_training_history(model_history, window_size, name):\n",
    "    for i, history in enumerate(model_history):\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
    "        plt.title(f'Model Loss for {name} {window_size[i]} Days')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    #plot prediction vs true value\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot prediction vs true value\n",
    "def plot_predictions(model_history, X_tests, y_tests, window_size, name):\n",
    "    for id in range(len(model_history)):\n",
    "        y_pred = model_history[id].predict(X_tests[id])\n",
    "        days = range(1, output_days + 1)\n",
    "        \n",
    "        for i in range(0, len(y_pred), 1000):\n",
    "            plt.figure(figsize=(14, 7))\n",
    "            plt.plot(days, y_tests[id][i, :, 0], marker='o', label='True Min', color='blue')\n",
    "            plt.plot(days, y_pred[i, :, 0], marker='o', label='Predicted Min', color='red')\n",
    "            plt.plot(days, y_tests[id][i, :, 1], marker='o', label='True Max', color='green')\n",
    "            plt.plot(days, y_pred[i, :, 1], marker='o', label='Predicted Max', color='orange')\n",
    "            plt.title(f'Min and Max Predictions for {name} {window_size[id]} Days')\n",
    "            plt.ylabel('Price')\n",
    "            plt.xlabel('Day')\n",
    "            plt.legend()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '20250204' is ready.\n",
      "Data for 14 days loaded.\n",
      "Data split into training and testing sets.\n",
      "start training:\n",
      "\n",
      "Epoch 1/20\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 895.1517 - mae: 28.2823 - val_loss: 329.7523 - val_mae: 14.5197\n",
      "Epoch 2/20\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 239.5216 - mae: 12.0781 - val_loss: 195.7143 - val_mae: 10.0716\n",
      "Epoch 3/20\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 188.2824 - mae: 9.7738 - val_loss: 183.8684 - val_mae: 9.8165\n",
      "Epoch 4/20\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 175.0577 - mae: 9.4497 - val_loss: 183.0956 - val_mae: 9.8543\n",
      "Epoch 5/20\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 168.8484 - mae: 9.3087 - val_loss: 183.0467 - val_mae: 9.8650\n",
      "Epoch 6/20\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 179.2630 - mae: 9.6830 - val_loss: 183.0489 - val_mae: 9.9027\n",
      "Epoch 7/20\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 193.9899 - mae: 9.9085 - val_loss: 183.4745 - val_mae: 9.8289\n",
      "Epoch 8/20\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 162.5561 - mae: 9.3211 - val_loss: 183.0271 - val_mae: 9.8949\n",
      "Epoch 9/20\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 202.1315 - mae: 10.1847 - val_loss: 183.2934 - val_mae: 9.8362\n",
      "Epoch 10/20\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 168.3617 - mae: 9.2983 - val_loss: 183.2054 - val_mae: 9.8426\n",
      "Epoch 11/20\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 174.0420 - mae: 9.5101 - val_loss: 183.0459 - val_mae: 9.8652\n",
      "Epoch 12/20\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 199.5008 - mae: 9.9494 - val_loss: 183.0833 - val_mae: 9.8570\n",
      "Epoch 13/20\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 193.8629 - mae: 9.7511 - val_loss: 183.3035 - val_mae: 9.8355\n",
      "Epoch 14/20\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 174.4671 - mae: 9.5719 - val_loss: 183.1278 - val_mae: 9.8863\n",
      "Epoch 15/20\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 178.2326 - mae: 9.3986 - val_loss: 116.9300 - val_mae: 7.0288\n",
      "Epoch 16/20\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 118.7393 - mae: 7.0374 - val_loss: 92.2470 - val_mae: 6.0776\n",
      "Epoch 17/20\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 91.1401 - mae: 5.7697 - val_loss: 100.6377 - val_mae: 6.8926\n",
      "Epoch 18/20\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 73.8271 - mae: 5.4120 - val_loss: 66.8045 - val_mae: 5.4385\n",
      "Epoch 19/20\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 56.9277 - mae: 4.8040 - val_loss: 49.6164 - val_mae: 4.2949\n",
      "Epoch 20/20\n",
      "\u001b[1m223/223\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 41.6899 - mae: 3.9308 - val_loss: 51.0033 - val_mae: 4.9631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained on 14 days has been saved.\n",
      "Data for 30 days loaded.\n",
      "Data split into training and testing sets.\n",
      "start training:\n",
      "\n",
      "Epoch 1/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - loss: 838.4885 - mae: 27.7495 - val_loss: 323.2439 - val_mae: 14.5174\n",
      "Epoch 2/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - loss: 236.2202 - mae: 12.0343 - val_loss: 187.0585 - val_mae: 9.9811\n",
      "Epoch 3/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - loss: 177.3891 - mae: 9.7933 - val_loss: 175.4630 - val_mae: 9.7109\n",
      "Epoch 4/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 158.7457 - mae: 9.3005 - val_loss: 174.4291 - val_mae: 9.7358\n",
      "Epoch 5/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - loss: 169.4814 - mae: 9.5348 - val_loss: 174.8581 - val_mae: 9.7174\n",
      "Epoch 6/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - loss: 163.4765 - mae: 9.4251 - val_loss: 174.4507 - val_mae: 9.7333\n",
      "Epoch 7/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - loss: 180.1530 - mae: 9.8126 - val_loss: 174.5148 - val_mae: 9.7294\n",
      "Epoch 8/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - loss: 165.5625 - mae: 9.5317 - val_loss: 174.5486 - val_mae: 9.7283\n",
      "Epoch 9/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - loss: 153.2445 - mae: 9.2436 - val_loss: 174.3027 - val_mae: 9.7714\n",
      "Epoch 10/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - loss: 165.9617 - mae: 9.3503 - val_loss: 174.9449 - val_mae: 9.7153\n",
      "Epoch 11/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - loss: 154.2126 - mae: 9.2698 - val_loss: 174.5002 - val_mae: 9.7299\n",
      "Epoch 12/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - loss: 174.0633 - mae: 9.6128 - val_loss: 174.6658 - val_mae: 9.7216\n",
      "Epoch 13/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - loss: 180.2335 - mae: 9.4331 - val_loss: 137.1476 - val_mae: 8.0471\n",
      "Epoch 14/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - loss: 121.0751 - mae: 7.5559 - val_loss: 105.5867 - val_mae: 6.9073\n",
      "Epoch 15/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - loss: 103.1117 - mae: 6.6354 - val_loss: 103.5488 - val_mae: 7.7189\n",
      "Epoch 16/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - loss: 72.2018 - mae: 5.8861 - val_loss: 66.5752 - val_mae: 5.3996\n",
      "Epoch 17/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - loss: 72.0282 - mae: 5.7517 - val_loss: 62.1181 - val_mae: 5.3746\n",
      "Epoch 18/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - loss: 57.1924 - mae: 5.1135 - val_loss: 72.5920 - val_mae: 6.2106\n",
      "Epoch 19/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - loss: 66.9028 - mae: 5.7297 - val_loss: 57.8871 - val_mae: 5.1159\n",
      "Epoch 20/20\n",
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - loss: 53.4422 - mae: 4.9194 - val_loss: 49.6588 - val_mae: 4.7669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained on 30 days has been saved.\n",
      "Data for 60 days loaded.\n",
      "Data split into training and testing sets.\n",
      "start training:\n",
      "\n",
      "Epoch 1/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - loss: 824.7623 - mae: 27.8427 - val_loss: 304.6387 - val_mae: 14.5680\n",
      "Epoch 2/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 45ms/step - loss: 218.8359 - mae: 12.0829 - val_loss: 170.6193 - val_mae: 9.8068\n",
      "Epoch 3/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 45ms/step - loss: 185.2769 - mae: 9.7237 - val_loss: 159.2126 - val_mae: 9.4789\n",
      "Epoch 4/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 45ms/step - loss: 142.2570 - mae: 9.0663 - val_loss: 158.2343 - val_mae: 9.4883\n",
      "Epoch 5/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - loss: 171.0633 - mae: 9.6445 - val_loss: 158.4848 - val_mae: 9.4814\n",
      "Epoch 6/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - loss: 136.4718 - mae: 9.0587 - val_loss: 158.0541 - val_mae: 9.5031\n",
      "Epoch 7/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 45ms/step - loss: 168.0379 - mae: 9.7588 - val_loss: 158.2751 - val_mae: 9.4864\n",
      "Epoch 8/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - loss: 166.2074 - mae: 9.5529 - val_loss: 159.0700 - val_mae: 9.4780\n",
      "Epoch 9/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - loss: 160.7972 - mae: 9.3821 - val_loss: 158.2531 - val_mae: 9.4875\n",
      "Epoch 10/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 44ms/step - loss: 154.6334 - mae: 9.2351 - val_loss: 158.2273 - val_mae: 9.4895\n",
      "Epoch 11/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 45ms/step - loss: 156.2073 - mae: 9.2338 - val_loss: 158.3026 - val_mae: 9.4859\n",
      "Epoch 12/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 45ms/step - loss: 161.8205 - mae: 9.6899 - val_loss: 158.1696 - val_mae: 9.4914\n",
      "Epoch 13/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 44ms/step - loss: 159.7530 - mae: 9.3867 - val_loss: 158.6203 - val_mae: 9.4794\n",
      "Epoch 14/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 44ms/step - loss: 153.6966 - mae: 9.3318 - val_loss: 158.4978 - val_mae: 9.4809\n",
      "Epoch 15/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - loss: 143.3673 - mae: 9.0385 - val_loss: 158.0135 - val_mae: 9.5191\n",
      "Epoch 16/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 45ms/step - loss: 164.0848 - mae: 9.5625 - val_loss: 151.6887 - val_mae: 9.1291\n",
      "Epoch 17/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 44ms/step - loss: 120.5408 - mae: 7.7057 - val_loss: 92.8729 - val_mae: 6.8765\n",
      "Epoch 18/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 45ms/step - loss: 90.6425 - mae: 6.7202 - val_loss: 79.2820 - val_mae: 6.2932\n",
      "Epoch 19/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - loss: 62.3410 - mae: 5.7387 - val_loss: 81.2914 - val_mae: 6.3157\n",
      "Epoch 20/20\n",
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 45ms/step - loss: 63.1807 - mae: 5.5811 - val_loss: 65.2391 - val_mae: 5.7370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained on 60 days has been saved.\n",
      "Data for 90 days loaded.\n",
      "Data split into training and testing sets.\n",
      "start training:\n",
      "\n",
      "Epoch 1/20\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 72ms/step - loss: 764.2434 - mae: 26.9715 - val_loss: 287.0748 - val_mae: 14.0790\n",
      "Epoch 2/20\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 68ms/step - loss: 229.4497 - mae: 11.9757 - val_loss: 159.8993 - val_mae: 9.6388\n",
      "Epoch 3/20\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 68ms/step - loss: 158.8000 - mae: 9.5729 - val_loss: 149.1072 - val_mae: 9.3366\n",
      "Epoch 4/20\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 66ms/step - loss: 142.1436 - mae: 9.0206 - val_loss: 148.3455 - val_mae: 9.3349\n",
      "Epoch 5/20\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 69ms/step - loss: 139.8774 - mae: 9.1504 - val_loss: 148.1326 - val_mae: 9.3393\n",
      "Epoch 6/20\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 66ms/step - loss: 135.6909 - mae: 9.0294 - val_loss: 148.2936 - val_mae: 9.3359\n",
      "Epoch 7/20\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 67ms/step - loss: 143.4933 - mae: 9.2125 - val_loss: 148.4612 - val_mae: 9.3342\n",
      "Epoch 8/20\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 69ms/step - loss: 140.8080 - mae: 9.1615 - val_loss: 148.5423 - val_mae: 9.3336\n",
      "Epoch 9/20\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 68ms/step - loss: 165.3033 - mae: 9.5492 - val_loss: 149.6862 - val_mae: 9.3421\n",
      "Epoch 10/20\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 67ms/step - loss: 124.9370 - mae: 8.8203 - val_loss: 148.1970 - val_mae: 9.3372\n",
      "Epoch 11/20\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 66ms/step - loss: 148.3344 - mae: 9.3332 - val_loss: 148.4599 - val_mae: 9.3338\n",
      "Epoch 12/20\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 66ms/step - loss: 159.8074 - mae: 9.7316 - val_loss: 143.3171 - val_mae: 8.9765\n",
      "Epoch 13/20\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 72ms/step - loss: 127.5272 - mae: 8.2158 - val_loss: 92.7261 - val_mae: 6.7261\n",
      "Epoch 14/20\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 66ms/step - loss: 88.6643 - mae: 6.6506 - val_loss: 74.6028 - val_mae: 6.2298\n",
      "Epoch 15/20\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 64ms/step - loss: 64.5948 - mae: 5.7841 - val_loss: 64.3490 - val_mae: 5.7318\n",
      "Epoch 16/20\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 64ms/step - loss: 55.9231 - mae: 5.3624 - val_loss: 56.6385 - val_mae: 5.4098\n",
      "Epoch 17/20\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 68ms/step - loss: 49.5261 - mae: 5.0176 - val_loss: 48.7852 - val_mae: 4.7598\n",
      "Epoch 18/20\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 65ms/step - loss: 42.3301 - mae: 4.4350 - val_loss: 47.6558 - val_mae: 5.3034\n",
      "Epoch 19/20\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 67ms/step - loss: 40.6094 - mae: 4.3904 - val_loss: 34.2954 - val_mae: 3.9401\n",
      "Epoch 20/20\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 33.2846 - mae: 4.0828"
     ]
    }
   ],
   "source": [
    "function_list = [baseline_LSTM_model, \n",
    "                 #baseline_GRU_model, \n",
    "                 #CNN_LSTM_model, \n",
    "                 #CNN_GRU_model, \n",
    "                 #CNN_LSTM_SA_model, \n",
    "                 #CNN_GRU_SA_model,\n",
    "                 #TFT_transformer_model\n",
    "                 ]\n",
    "for model_function in function_list:\n",
    "    models, model_history, name, X_tests, y_tests = base_training(model_function, \n",
    "                                                        loading_path = \"data/\", \n",
    "                                                        saving_path=\"saved models/\",\n",
    "                                                        version_name=\"20250204\",\n",
    "                                                        seed = 42,\n",
    "                                                        test_size = 0.8,\n",
    "                                                        epochs = 20,\n",
    "                                                        batch_size = 8,\n",
    "                                                        shuffle=True, \n",
    "                                                        metrics=['mae'],\n",
    "                                                        #loss = 'mean_squared_error',\n",
    "                                                        loss = custom_loss,\n",
    "                                                        optimizer='adam',\n",
    "                                                        window_size = window_size, \n",
    "                                                        num_features = num_features, \n",
    "                                                        output_days = output_days, \n",
    "                                                        output_features = output_features\n",
    "                                                        )\n",
    "    plot_training_history(model_history, window_size, name)\n",
    "    plot_predictions(models, X_tests, y_tests, window_size, name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build ensemble model with meta mode (transfer learning/fine-tuning), since will do back propaggation through the ensemble layer all the way to the individual model, we can use multiple stock training data for the individual model, increase the generalization, and do the tuning with the target data, in the ensemble layer training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "def ensemble_stacking (models_inputs, models_outputs, output_days, output_features, optimizer, loss, metrics):\n",
    "    merged_output = layers.concatenate(models_outputs, axis=-1)\n",
    "    #searching method for removing the last layer of the model, and directly inputting weight into stackinng model\n",
    "    \n",
    "    #stacking model\n",
    "    merged_output = layers.Dense(64, activation='relu')(merged_output)\n",
    "    merged_output = layers.Dense(output_days * output_features)(merged_output)\n",
    "    final_output = layers.Reshape((output_days, output_features))(merged_output)\n",
    "    ensemble_model = Model(inputs=models_inputs, outputs=final_output)\n",
    "    ensemble_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return ensemble_model\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_weighting (models_inputs, models_outputs, output_days, output_features, optimizer, loss, metrics):\n",
    "    merged_output = layers.Add()(models_outputs)\n",
    "    merged_output = layers.Dense(output_days*output_features)(merged_output)\n",
    "    final_output = layers.Reshape((output_days, output_features))(merged_output)\n",
    "    ensemble_model = Model(inputs=models_inputs, outputs=final_output)\n",
    "    ensemble_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return ensemble_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need more understanding on the MoE\n",
    "''' stacking is y_p = sum(w*y_i)\n",
    "    MoE is y_p = sum(g_i*y_i)\n",
    "    can add other function model, like anomaly detection, to the ensemble model, since most of the data is normal, the model can be used to detect the anomaly data(Event)\n",
    "'''\n",
    "\n",
    "def ensemble_MoE(models_inputs, models_outputs, output_days, output_features, optimizer, loss, metrics):\n",
    "    # Step 1: Concatenate the outputs of all the experts\n",
    "    concatenated_experts = layers.concatenate(models_outputs)\n",
    "    \n",
    "    # Step 2: Apply a gate to weight the expert outputs\n",
    "    gate = layers.Dense(len(models_outputs), activation='softmax')(concatenated_experts)\n",
    "    \n",
    "    # Step 3: Multiply the gate with the expert outputs\n",
    "    weighted_experts = [layers.Multiply()([gate[:, i:i+1], models_outputs[i]]) for i in range(len(models_outputs))]\n",
    "    \n",
    "    # Step 4: Sum the weighted expert outputs\n",
    "    final_expert_output = layers.Add()(weighted_experts)\n",
    "    \n",
    "    # Step 5: Reshape the final output to the desired shape\n",
    "    final_output = layers.Reshape((output_days, output_features))(final_expert_output)\n",
    "\n",
    "    # Step 6: Define the ensemble model\n",
    "    ensemble_model = models.Model(inputs=models_inputs, outputs=final_output)\n",
    "\n",
    "    # Step 7: Compile the model\n",
    "    ensemble_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    return ensemble_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to load models in provided path\n",
    "from tensorflow.keras.models import load_model\n",
    "def load_models(path, remove_last_layer=True):\n",
    "    orginal_models = []\n",
    "    models_outputs = []\n",
    "    models_inputs = []\n",
    "    models_order = []\n",
    "    for file in sorted(os.listdir(path)):\n",
    "        if file.endswith(\".h5\"):\n",
    "            model = load_model(os.path.join(path, file))\n",
    "            models_order.append(file)\n",
    "            print(f\"Model loaded: {os.path.join(path, file)}\")\n",
    "            orginal_models.append(load_model(os.path.join(path, file), compile=False))\n",
    "            if remove_last_layer:\n",
    "                try:\n",
    "                    output = model.layers[-2].output  # Use the second last layer as output\n",
    "                    model = Model(inputs=model.input, outputs=output)\n",
    "                    print(f\"Removed final layer from model: {os.path.join(path, file)}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error modifying model {os.path.join(path, file)}: {e}\")\n",
    "            models_inputs.append(model.input)\n",
    "            print(models_inputs)\n",
    "            models_outputs.append(model.output)\n",
    "    \n",
    "    \n",
    "    return orginal_models, models_inputs, models_outputs, models_order\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_model_training(models_inputs, models_outputs, output_days, output_features, model_type = \"stacking\", optimizer='adam', loss='mean_squared_error', metrics=['mae']):\n",
    "    if model_type == 'stacking':\n",
    "        ensemble_model = ensemble_stacking(models_inputs, models_outputs, output_days, output_features, optimizer, loss, metrics)\n",
    "    elif model_type == 'weighting':\n",
    "        ensemble_model = ensemble_weighting(models_inputs, models_outputs, output_days, output_features, optimizer, loss, metrics)\n",
    "    elif model_type == 'MoE':\n",
    "        ensemble_model = ensemble_MoE(models_inputs, models_outputs, output_days, output_features, optimizer, loss, metrics)\n",
    "    else:\n",
    "        print(\"Please provide a valid model type.\")\n",
    "        return None\n",
    "    return ensemble_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_GRU_SA_model_14days.h5\n",
      "Removed final layer from model: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_GRU_SA_model_14days.h5\n",
      "[<KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_45>]\n",
      "Model loaded: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_GRU_SA_model_30days.h5\n",
      "Removed final layer from model: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_GRU_SA_model_30days.h5\n",
      "[<KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_45>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_46>]\n",
      "Model loaded: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_GRU_SA_model_60days.h5\n",
      "Removed final layer from model: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_GRU_SA_model_60days.h5\n",
      "[<KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_45>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_46>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_47>]\n",
      "Model loaded: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_GRU_SA_model_7days.h5\n",
      "Removed final layer from model: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_GRU_SA_model_7days.h5\n",
      "[<KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_45>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_46>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_47>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_44>]\n",
      "Model loaded: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_GRU_model_14days.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed final layer from model: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_GRU_model_14days.h5\n",
      "[<KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_45>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_46>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_47>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_44>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_37>]\n",
      "Model loaded: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_GRU_model_30days.h5\n",
      "Removed final layer from model: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_GRU_model_30days.h5\n",
      "[<KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_45>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_46>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_47>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_44>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_37>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_38>]\n",
      "Model loaded: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_GRU_model_60days.h5\n",
      "Removed final layer from model: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_GRU_model_60days.h5\n",
      "[<KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_45>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_46>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_47>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_44>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_37>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_38>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_39>]\n",
      "Model loaded: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_GRU_model_7days.h5\n",
      "Removed final layer from model: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_GRU_model_7days.h5\n",
      "[<KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_45>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_46>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_47>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_44>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_37>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_38>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_39>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_36>]\n",
      "Model loaded: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_LSTM_SA_model_14days.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed final layer from model: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_LSTM_SA_model_14days.h5\n",
      "[<KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_45>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_46>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_47>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_44>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_37>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_38>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_39>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_36>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_41>]\n",
      "Model loaded: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_LSTM_SA_model_30days.h5\n",
      "Removed final layer from model: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_LSTM_SA_model_30days.h5\n",
      "[<KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_45>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_46>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_47>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_44>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_37>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_38>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_39>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_36>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_41>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_42>]\n",
      "Model loaded: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_LSTM_SA_model_60days.h5\n",
      "Removed final layer from model: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_LSTM_SA_model_60days.h5\n",
      "[<KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_45>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_46>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_47>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_44>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_37>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_38>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_39>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_36>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_41>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_42>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_43>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_LSTM_SA_model_7days.h5\n",
      "Removed final layer from model: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_LSTM_SA_model_7days.h5\n",
      "[<KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_45>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_46>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_47>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_44>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_37>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_38>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_39>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_36>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_41>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_42>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_43>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_40>]\n",
      "Model loaded: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_LSTM_model_14days.h5\n",
      "Removed final layer from model: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_LSTM_model_14days.h5\n",
      "[<KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_45>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_46>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_47>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_44>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_37>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_38>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_39>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_36>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_41>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_42>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_43>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_40>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_33>]\n",
      "Model loaded: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_LSTM_model_30days.h5\n",
      "Removed final layer from model: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_LSTM_model_30days.h5\n",
      "[<KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_45>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_46>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_47>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_44>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_37>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_38>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_39>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_36>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_41>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_42>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_43>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_40>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_33>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_34>]\n",
      "Model loaded: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_LSTM_model_60days.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed final layer from model: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_LSTM_model_60days.h5\n",
      "[<KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_45>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_46>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_47>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_44>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_37>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_38>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_39>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_36>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_41>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_42>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_43>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_40>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_33>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_34>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_35>]\n",
      "Model loaded: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_LSTM_model_7days.h5\n",
      "Removed final layer from model: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/CNN_LSTM_model_7days.h5\n",
      "[<KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_45>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_46>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_47>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_44>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_37>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_38>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_39>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_36>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_41>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_42>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_43>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_40>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_33>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_34>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_35>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_32>]\n",
      "Model loaded: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/baseline_GRU_model_14days.h5\n",
      "Removed final layer from model: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/baseline_GRU_model_14days.h5\n",
      "[<KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_45>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_46>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_47>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_44>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_37>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_38>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_39>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_36>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_41>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_42>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_43>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_40>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_33>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_34>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_35>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_32>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_53>]\n",
      "Model loaded: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/baseline_GRU_model_30days.h5\n",
      "Removed final layer from model: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/baseline_GRU_model_30days.h5\n",
      "[<KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_45>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_46>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_47>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_44>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_37>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_38>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_39>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_36>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_41>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_42>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_43>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_40>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_33>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_34>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_35>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_32>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_53>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_30>]\n",
      "Model loaded: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/baseline_GRU_model_60days.h5\n",
      "Removed final layer from model: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/baseline_GRU_model_60days.h5\n",
      "[<KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_45>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_46>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_47>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_44>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_37>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_38>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_39>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_36>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_41>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_42>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_43>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_40>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_33>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_34>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_35>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_32>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_53>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_30>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_31>]\n",
      "Model loaded: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/baseline_GRU_model_7days.h5\n",
      "Removed final layer from model: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/baseline_GRU_model_7days.h5\n",
      "[<KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_45>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_46>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_47>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_44>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_37>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_38>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_39>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_36>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_41>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_42>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_43>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_40>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_33>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_34>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_35>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_32>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_53>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_30>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_31>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_52>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/baseline_LSTM_model_14days.h5\n",
      "Removed final layer from model: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/baseline_LSTM_model_14days.h5\n",
      "[<KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_45>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_46>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_47>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_44>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_37>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_38>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_39>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_36>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_41>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_42>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_43>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_40>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_33>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_34>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_35>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_32>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_53>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_30>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_31>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_52>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_49>]\n",
      "Model loaded: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/baseline_LSTM_model_30days.h5\n",
      "Removed final layer from model: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/baseline_LSTM_model_30days.h5\n",
      "[<KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_45>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_46>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_47>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_44>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_37>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_38>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_39>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_36>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_41>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_42>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_43>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_40>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_33>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_34>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_35>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_32>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_53>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_30>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_31>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_52>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_49>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_50>]\n",
      "Model loaded: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/baseline_LSTM_model_60days.h5\n",
      "Removed final layer from model: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/baseline_LSTM_model_60days.h5\n",
      "[<KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_45>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_46>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_47>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_44>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_37>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_38>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_39>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_36>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_41>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_42>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_43>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_40>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_33>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_34>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_35>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_32>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_53>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_30>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_31>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_52>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_49>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_50>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_51>]\n",
      "Model loaded: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/baseline_LSTM_model_7days.h5\n",
      "Removed final layer from model: /Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam/baseline_LSTM_model_7days.h5\n",
      "[<KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_45>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_46>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_47>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_44>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_37>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_38>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_39>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_36>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_41>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_42>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_43>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_40>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_33>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_34>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_35>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_32>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_53>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_30>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_31>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_52>, <KerasTensor shape=(None, 14, 8), dtype=float32, sparse=False, name=input_layer_49>, <KerasTensor shape=(None, 30, 8), dtype=float32, sparse=False, name=input_layer_50>, <KerasTensor shape=(None, 60, 8), dtype=float32, sparse=False, name=input_layer_51>, <KerasTensor shape=(None, 7, 8), dtype=float32, sparse=False, name=input_layer_48>]\n"
     ]
    }
   ],
   "source": [
    "#load the whole folder of models (you might have multiple floders of models, then need multiple load_models function and do the selection in the next cell)\n",
    "orginal_models, models_inputs, models_outputs, models_order = load_models(\"/Users/hoyinchui/Desktop/tbot-st-ta/saved models/20250128/_s-42_t-0.8_e-20_b-8_S-True_m-['mae']_l-mean_squared_error_o-adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CNN_GRU_SA_model_14days.h5', 'CNN_GRU_SA_model_30days.h5', 'CNN_GRU_SA_model_60days.h5', 'CNN_GRU_SA_model_7days.h5', 'CNN_GRU_model_14days.h5', 'CNN_GRU_model_30days.h5', 'CNN_GRU_model_60days.h5', 'CNN_GRU_model_7days.h5', 'CNN_LSTM_SA_model_14days.h5', 'CNN_LSTM_SA_model_30days.h5', 'CNN_LSTM_SA_model_60days.h5', 'CNN_LSTM_SA_model_7days.h5', 'CNN_LSTM_model_14days.h5', 'CNN_LSTM_model_30days.h5', 'CNN_LSTM_model_60days.h5', 'CNN_LSTM_model_7days.h5', 'baseline_GRU_model_14days.h5', 'baseline_GRU_model_30days.h5', 'baseline_GRU_model_60days.h5', 'baseline_GRU_model_7days.h5', 'baseline_LSTM_model_14days.h5', 'baseline_LSTM_model_30days.h5', 'baseline_LSTM_model_60days.h5', 'baseline_LSTM_model_7days.h5']\n"
     ]
    }
   ],
   "source": [
    "#select the models(4 models Time-based) and ensemble method you want and feed into the ensemble traing function\n",
    "#if stargetry changed, the function and training set need to updatye too \n",
    "print(models_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#be aware of the order, it need to be 7, 14, 30, 60 from small to large\n",
    "models_inputs_selected = [models_inputs[3], models_inputs[0], models_inputs[1], models_inputs[2]]   \n",
    "models_outputs_selected = [models_outputs[3], models_outputs[0], models_outputs[1], models_outputs[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the models(4 models Time-based) and ensemble method you want and feed into the ensemble traing function\n",
    "#if stargetry changed, the function and training set need to updatye too \n",
    "ensemble_s_model = ensemble_model_training(models_inputs_selected, models_outputs_selected, output_days, output_features, model_type = \"stacking\", optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_w_model = ensemble_model_training(models_inputs_selected, models_outputs_selected, output_days, output_features, model_type = \"weighting\", optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_MoE_model = ensemble_model_training(models_inputs_selected, models_outputs_selected, output_days, output_features, model_type = \"MoE\", optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data for testing in ensemble model\n",
    "\n",
    "def load_data(window_size = window_size, path = f\"/Users/hoyinchui/Desktop/tbot-st-ta/Testing data/GLD_model_testing_data_pre_ens_Ai1_v3_pkl\", test_size = 0.8, shuffle = True, seed = 42):\n",
    "    all_X_train = []\n",
    "    all_X_test = []\n",
    "    all_y_train = []\n",
    "    all_y_test = []\n",
    "    \n",
    "    y = pd.read_pickle(f\"{path}/y_a.pkl\")\n",
    "    for window in window_size:\n",
    "        X = pd.read_pickle(f\"{path}/X_{window}days_a.pkl\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=shuffle, random_state=seed)\n",
    "        all_X_train.append(X_train)\n",
    "        all_X_test.append(X_test)\n",
    "        all_y_train.append(y_train)\n",
    "        all_y_test.append(y_test)\n",
    "    return all_X_train, all_X_test, all_y_train, all_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X_train, all_X_test, all_y_train, all_y_test = load_data(path = f\"/Users/hoyinchui/Desktop/tbot-st-ta/Testing data/GLD_model_testing_data_pre_ens_Ai1_v3_pkl\", test_size = 0.8, shuffle = True, seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the ensemble model\n",
    "def ensemble_model_fit(ensemble_model, all_X_train, all_y_train, epochs, batch_size, save_model_path, folder_name):\n",
    "    history = ensemble_model.fit(all_X_train, all_y_train[0], epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    #create floder to save the model\n",
    "    save_model_path = os.path.join(save_model_path, folder_name)\n",
    "    # Save the model\n",
    "    params = f\"ensemble_e-{epochs}_b-{batch_size}\"\n",
    "    os.makedirs(os.path.join(save_model_path, params ), exist_ok=True)\n",
    "    name = ensemble_model.name\n",
    "    ensemble_model.save(os.path.join(os.path.join(save_model_path, folder_name, params ),f\"{name}.h5\"))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 2.7000e-04 - mae: 0.0122\n",
      "Epoch 2/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 2.9753e-04 - mae: 0.0126\n",
      "Epoch 3/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 2.5256e-04 - mae: 0.0116\n",
      "Epoch 4/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 2.4424e-04 - mae: 0.0114\n",
      "Epoch 5/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 2.6069e-04 - mae: 0.0120\n",
      "Epoch 6/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 2.5150e-04 - mae: 0.0116\n",
      "Epoch 7/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 3.6657e-04 - mae: 0.0144\n",
      "Epoch 8/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 2.3350e-04 - mae: 0.0113\n",
      "Epoch 9/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 3.0335e-04 - mae: 0.0132\n",
      "Epoch 10/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 2.5268e-04 - mae: 0.0118\n",
      "Epoch 11/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 2.2285e-04 - mae: 0.0110\n",
      "Epoch 12/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 2.4682e-04 - mae: 0.0116\n",
      "Epoch 13/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 2.3954e-04 - mae: 0.0115\n",
      "Epoch 14/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 2.4749e-04 - mae: 0.0115\n",
      "Epoch 15/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 3.4386e-04 - mae: 0.0138\n",
      "Epoch 16/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 2.5635e-04 - mae: 0.0119\n",
      "Epoch 17/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 2.9021e-04 - mae: 0.0127\n",
      "Epoch 18/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 2.6445e-04 - mae: 0.0122\n",
      "Epoch 19/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 3.3119e-04 - mae: 0.0137\n",
      "Epoch 20/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 2.5977e-04 - mae: 0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "#traing the ensemble model\n",
    "ensemble_s_model_history = ensemble_model_fit( ensemble_s_model, all_X_train, all_y_train, epochs = 20, batch_size = 8, save_model_path = \"/Users/hoyinchui/Desktop/tbot-st-ta/saved models\",folder_name  = \"20250202\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the following 3 cell can skip, theey are old version for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model and predict and save the prediction\n",
    "#for window in window_size:\n",
    "#    model = tf.keras.models.load_model(f\"CNN_LSTM_{window}days.h5\")\n",
    "#    y_pred = model.predict(X_test)\n",
    "#    pd.DataFrame(y_pred).to_csv(f\"y_pred_CNN_LSTM_{window}days.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting all windows prediction into the dataframe as X\n",
    "#import numpy as np\n",
    "#X_ensemble = pd.DataFrame()\n",
    "#for window in window_size:\n",
    "#    X_ensemble = pd.concat([X_ensemble, pd.read_csv(f\"y_pred_CNN_LSTM_{window}days.csv\")], axis=1)\n",
    "#    #combein the columns, so that can be used as input for the ensemble model\n",
    "#   X_ensemble = X_ensemble.applymap(lambda x: np.vstack(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They can combein in 1 for loop, I just split it for clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train_test_split_ensemble(X,y,test_size=0.5, seed=42):    \n",
    "#    #split again for the prediction model and the ensemble model\n",
    "#    #Since we already shuffled the data, we can just split the data in half, and easaier to manage\n",
    "#    X_train_ensemble, X_test_ensemble, y_train_ensemble, y_test_ensemble = train_test_split(X, y, test_size=test_size, shuffle=False, random_state=seed)\n",
    "#    return X_train_ensemble, X_test_ensemble, y_train_ensemble, y_test_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_ensemble, X_test_ensemble, y_train_ensemble, y_test_ensemble = train_test_split_ensemble(X_ensemble, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Model (TBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.models import Model\n",
    "#def ensemble_CNN_LSTM(window_size, num_features):\n",
    "#    inputs = layers.Input(shape=(window_size, num_features))\n",
    "#    x = layers.Conv1D(filters=64, kernel_size=2, activation='relu')(inputs)\n",
    "#    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "#    x = layers.Conv1D(filters=128, kernel_size=2, activation='relu')(x)\n",
    "#    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "#    x = layers.LSTM(100)(x)\n",
    "#    model = Model(inputs=inputs, outputs=x)\n",
    "#    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def ensemble_s_models(window_size, num_features, output_days, output_features):\n",
    "#        models = []\n",
    "#    for window in window_size:\n",
    "#        model = ensemble_CNN_LSTM(window, num_features)\n",
    "#        models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def ensemble_MoE_models(window_size, num_features, output_days, output_features):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def ensemble_h_models(window_size, num_features, output_days, output_features):\n",
    "#    models = []\n",
    "#    for window in window_size:\n",
    "#        model = ensemble_CNN_LSTM(window, num_features)\n",
    "#        models.append(model)\n",
    "#    models_inputs = [model.input for model in models]\n",
    "#    models_outputs = [model.output for model in models]\n",
    "#   merged = layers.concatenate(models_outputs, axis=-1)\n",
    "#    merged_output = layers.Dense(output_days * output_features)(merged)\n",
    "#    final_output = layers.Reshape((output_days, output_features))(merged_output)\n",
    "#    ensemble_model = Model(inputs=models_inputs, outputs=final_output)\n",
    "#    ensemble_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "#\n",
    "#    return ensemble_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 0.0279 - mae: 0.1027 - val_loss: 5.5140e-04 - val_mae: 0.0176\n",
      "Epoch 2/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 5.2824e-04 - mae: 0.0169 - val_loss: 4.1390e-04 - val_mae: 0.0154\n",
      "Epoch 3/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 5.1239e-04 - mae: 0.0167 - val_loss: 4.6971e-04 - val_mae: 0.0162\n",
      "Epoch 4/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 4.3028e-04 - mae: 0.0155 - val_loss: 0.0011 - val_mae: 0.0263\n",
      "Epoch 5/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 7.6607e-04 - mae: 0.0210 - val_loss: 4.8477e-04 - val_mae: 0.0170\n",
      "Epoch 6/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 4.1826e-04 - mae: 0.0152 - val_loss: 3.3729e-04 - val_mae: 0.0135\n",
      "Epoch 7/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 3.9106e-04 - mae: 0.0147 - val_loss: 5.9622e-04 - val_mae: 0.0190\n",
      "Epoch 8/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 3.8556e-04 - mae: 0.0144 - val_loss: 3.2576e-04 - val_mae: 0.0133\n",
      "Epoch 9/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 3.8984e-04 - mae: 0.0145 - val_loss: 3.6576e-04 - val_mae: 0.0140\n",
      "Epoch 10/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 3.0942e-04 - mae: 0.0130 - val_loss: 0.0012 - val_mae: 0.0284\n",
      "Epoch 11/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 5.3522e-04 - mae: 0.0171 - val_loss: 3.7922e-04 - val_mae: 0.0143\n",
      "Epoch 12/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 4.1376e-04 - mae: 0.0154 - val_loss: 3.7486e-04 - val_mae: 0.0142\n",
      "Epoch 13/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 3.8007e-04 - mae: 0.0147 - val_loss: 3.5824e-04 - val_mae: 0.0140\n",
      "Epoch 14/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 3.6434e-04 - mae: 0.0141 - val_loss: 2.8506e-04 - val_mae: 0.0124\n",
      "Epoch 15/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 3.4567e-04 - mae: 0.0139 - val_loss: 6.9771e-04 - val_mae: 0.0211\n",
      "Epoch 16/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 3.9665e-04 - mae: 0.0147 - val_loss: 3.4478e-04 - val_mae: 0.0138\n",
      "Epoch 17/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 3.2562e-04 - mae: 0.0133 - val_loss: 2.6830e-04 - val_mae: 0.0118\n",
      "Epoch 18/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 3.8294e-04 - mae: 0.0148 - val_loss: 5.2331e-04 - val_mae: 0.0175\n",
      "Epoch 19/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 3.2525e-04 - mae: 0.0134 - val_loss: 7.1152e-04 - val_mae: 0.0207\n",
      "Epoch 20/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - loss: 4.0285e-04 - mae: 0.0149 - val_loss: 2.7196e-04 - val_mae: 0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#import pandas as pd\n",
    "#seed = 42\n",
    "#test_size = 0.8 # we need to consider for the ensemble model's training and ttesting data, since it cannot use the same training data\n",
    "#epochs = 20\n",
    "#batch_size = 8\n",
    "#shuffle=True\n",
    "#all_X_train = []\n",
    "#all_X_test = []\n",
    "#all_y_train = []\n",
    "#all_y_test = []\n",
    "#\n",
    "#y = pd.read_pickle(f\"/Users/hoyinchui/Downloads/y_a.pkl\")\n",
    "#for window in window_size:\n",
    "#    X = pd.read_pickle(f\"/Users/hoyinchui/Downloads/X_{window}days_a.pkl\")\n",
    "#    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=shuffle, random_state=seed)\n",
    "#    all_X_train.append(X_train)\n",
    "#    all_X_test.append(X_test)\n",
    "#    all_y_train.append(y_train)\n",
    "#    all_y_test.append(y_test)\n",
    "#\n",
    "##ensemble_model = ensemble_h_models(window_size, num_features, output_days, output_features)\n",
    "##history = ensemble_model.fit(all_X_train, all_y_train[0], epochs=epochs, batch_size=batch_size, validation_data=(all_X_test, all_y_test[0]), verbose=1)\n",
    "##ensemble_model.save(f\"ensemble_CNN_LSTM.h5\")\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
