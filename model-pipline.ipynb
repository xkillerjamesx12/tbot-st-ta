{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = [7,14,30,60]\n",
    "num_features = 8 #depends on how much features we have\n",
    "output_days = 7\n",
    "output_features = 2 #min and max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_LSTM_model(window_size, num_features, output_days, output_features):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.LSTM(100, input_shape=(window_size, num_features), return_sequences=False))\n",
    "    model.add(layers.LSTM(100))\n",
    "    model.add(layers.Dense(output_days * output_features))  # Output for 7 days * 2 features (min and max)\n",
    "    model.add(layers.Reshape((output_days, output_features)))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_GRU_model(window_size, num_features, output_days, output_features):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.GRU(100, input_shape=(window_size, num_features), return_sequences=False))\n",
    "    model.add(layers.Dense(output_days * output_features))  # Output for 7 days * 2 features (min and max)\n",
    "    model.add(layers.Reshape((output_days, output_features)))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CNN_LSTM mdoel is a combination of CNN and LSTM. CNN is used to extract the features from the input data and LSTM is used to support the sequence data.\n",
    "'''\n",
    "def CNN_LSTM_model(window_size, num_features, output_days, output_features):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(window_size, num_features)))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.LSTM(100), return_sequences=True)\n",
    "    model.add(layers.LSTM(100))\n",
    "    model.add(layers.Dense(output_days*output_features))  # 7 days * 2 targets (High, Low)\n",
    "    model.add(layers.Reshape((7, 2))) # Reshape output to (7, 2)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    return model\n",
    "#can use KAN, Attenion layer to adjust the weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN + GRU model\n",
    "def CNN_GRU_model(window_size, num_features, output_days, output_features):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(window_size, num_features)))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(layers.GRU(100, return_sequences=False))\n",
    "    \n",
    "    model.add(layers.Dense(output_days * output_features))  # Output for 7 days * 2 features (min and max)\n",
    "    \n",
    "    model.add(layers.Reshape((output_days, output_features)))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Attention\n",
    "def CNN_LSTM_SA_model(window_size, num_features, output_days, output_features):\n",
    "    inputs = layers.Input(shape=(window_size, num_features))\n",
    "    x = layers.Conv1D(filters=64, kernel_size=2, activation='relu')(inputs)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = layers.Conv1D(filters=128, kernel_size=2, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = layers.LSTM(100, return_sequences=True)(x)\n",
    "    x = layers.LSTM(100, return_sequences=True)(x)\n",
    "    \n",
    "    # Attention mechanism\n",
    "    attention = Attention()([x, x])\n",
    "    x = layers.Concatenate()([x, attention])\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    x = layers.Dense(output_days*output_features)(x)\n",
    "    outputs = layers.Reshape((output_days, output_features))(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_GRU_SA_model(window_size, num_features, output_days, output_features):\n",
    "    inputs = layers.Input(shape=(window_size, num_features))\n",
    "    \n",
    "    x = layers.Conv1D(filters=64, kernel_size=2, activation='relu')(inputs)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = layers.Conv1D(filters=128, kernel_size=2, activation='relu')(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = layers.GRU(100, return_sequences=True)(x)\n",
    "    \n",
    "    attention = layers.Attention()([x, x])  # Self-attention (query = value = x)\n",
    "    \n",
    "    x = layers.Dense(output_days * output_features)(attention)\n",
    "    \n",
    "    outputs = layers.Reshape((output_days, output_features))(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta Model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "seed = 42\n",
    "test_size = 0.8\n",
    "epochs = 20\n",
    "batch_size = 8\n",
    "shuffle=True\n",
    "for window in window_size:\n",
    "    model = CNN_LSTM_model(window, num_features, output_days, output_features)\n",
    "    X = pd.read_pickle(f\"/Users/hoyinchui/Downloads/GLD_model_testing_data_i_v5_pkl/X_{window}days_i.pkl\")\n",
    "    y = pd.read_pickle(f\"/Users/hoyinchui/Downloads/GLD_model_testing_data_i_v5_pkl/y_{window}days_i.pkl\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=shuffle, random_state=seed)\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=1)\n",
    "    #provide a validation data to see the performance\n",
    "    test_loss, test_mae = model.evaluate(X_test, y_test)\n",
    "    # Plot the loss and metrics during training\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    # Plot MAE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Training MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.title('MAE over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    model.save(f\"CNN_LSTM_{window}days.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build ensemble model with meta mode (transfer learning/fine-tuning), since will do back propaggation through the ensemble layer all the way to the individual model, we can use multiple stock training data for the individual model, increase the generalization, and do the tuning with the target data, in the ensemble layer training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "def ensemble_stacking (models,output_days, output_features):\n",
    "    models_inputs = [] \n",
    "    models_outputs = []\n",
    "    for model in models:\n",
    "        model = models.load_model(model)\n",
    "        models_inputs.append(model.input)\n",
    "        models_outputs.append(model.output)\n",
    "    merged_output = layers.concatenate(models_outputs, axis=-1)\n",
    "    #searching method for removing the last layer of the model, and directly inputting weight into stackinng model\n",
    "    \n",
    "    #stacking model\n",
    "    merged_output = layers.Dense(64, activation='relu')(merged_output)\n",
    "    final_output = layers.Reshape((output_days, output_features))(merged_output)\n",
    "    ensemble_model = Model(inputs=models_inputs, outputs=final_output)\n",
    "    ensemble_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    return ensemble_model\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_weighting (models,output_days, output_features):\n",
    "    models_inputs = [] \n",
    "    models_outputs = []\n",
    "    for model in models:\n",
    "        model = models.load_model(model)\n",
    "        models_inputs.append(model.input)\n",
    "        models_outputs.append(model.output)\n",
    "    merged_output = layers.Add()(models_outputs)\n",
    "    merged_output = layers.Dense(1)(merged_output)\n",
    "    final_output = layers.Reshape((output_days, output_features))(merged_output)\n",
    "    ensemble_model = Model(inputs=models_inputs, outputs=final_output)\n",
    "    ensemble_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    return ensemble_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need more understanding on the MoE\n",
    "''' stacking is y_p = sum(w*y_i)\n",
    "    MoE is y_p = sum(g_i*y_i)\n",
    "    can add other function model, like anomaly detection, to the ensemble model, since most of the data is normal, the model can be used to detect the anomaly data(Event)\n",
    "'''\n",
    "def ensemble_MoE (models,output_days, output_features):\n",
    "    models_inputs = [] \n",
    "    models_outputs = []\n",
    "    for model in models:\n",
    "        model = models.load_model(model)\n",
    "        models_inputs.append(model.input)\n",
    "        models_outputs.append(model.output)\n",
    "    merged_output = layers.Add()(models_outputs)\n",
    "    gate = layers.Dense(len(models), activation='softmax')(merged_output)\n",
    "    expert_outputs = layers.Multiply()([gate, models_outputs])\n",
    "    final_output = layers.Reshape((output_days, output_features))(expert_outputs)\n",
    "    ensemble_model = Model(inputs=models_inputs, outputs=final_output)\n",
    "    ensemble_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    return ensemble_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the following 3 cell can skip, theey are old version for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model and predict and save the prediction\n",
    "#for window in window_size:\n",
    "#    model = tf.keras.models.load_model(f\"CNN_LSTM_{window}days.h5\")\n",
    "#    y_pred = model.predict(X_test)\n",
    "#    pd.DataFrame(y_pred).to_csv(f\"y_pred_CNN_LSTM_{window}days.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting all windows prediction into the dataframe as X\n",
    "#import numpy as np\n",
    "#X_ensemble = pd.DataFrame()\n",
    "#for window in window_size:\n",
    "#    X_ensemble = pd.concat([X_ensemble, pd.read_csv(f\"y_pred_CNN_LSTM_{window}days.csv\")], axis=1)\n",
    "#    #combein the columns, so that can be used as input for the ensemble model\n",
    "#   X_ensemble = X_ensemble.applymap(lambda x: np.vstack(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They can combein in 1 for loop, I just split it for clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train_test_split_ensemble(X,y,test_size=0.5, seed=42):    \n",
    "#    #split again for the prediction model and the ensemble model\n",
    "#    #Since we already shuffled the data, we can just split the data in half, and easaier to manage\n",
    "#    X_train_ensemble, X_test_ensemble, y_train_ensemble, y_test_ensemble = train_test_split(X, y, test_size=test_size, shuffle=False, random_state=seed)\n",
    "#    return X_train_ensemble, X_test_ensemble, y_train_ensemble, y_test_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_ensemble, X_test_ensemble, y_train_ensemble, y_test_ensemble = train_test_split_ensemble(X_ensemble, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Model (TBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.models import Model\n",
    "#def ensemble_CNN_LSTM(window_size, num_features):\n",
    "#    inputs = layers.Input(shape=(window_size, num_features))\n",
    "#    x = layers.Conv1D(filters=64, kernel_size=2, activation='relu')(inputs)\n",
    "#    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "#    x = layers.Conv1D(filters=128, kernel_size=2, activation='relu')(x)\n",
    "#    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "#    x = layers.LSTM(100)(x)\n",
    "#    model = Model(inputs=inputs, outputs=x)\n",
    "#    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def ensemble_s_models(window_size, num_features, output_days, output_features):\n",
    "#        models = []\n",
    "#    for window in window_size:\n",
    "#        model = ensemble_CNN_LSTM(window, num_features)\n",
    "#        models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def ensemble_MoE_models(window_size, num_features, output_days, output_features):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def ensemble_h_models(window_size, num_features, output_days, output_features):\n",
    "#    models = []\n",
    "#    for window in window_size:\n",
    "#        model = ensemble_CNN_LSTM(window, num_features)\n",
    "#        models.append(model)\n",
    "#    models_inputs = [model.input for model in models]\n",
    "#    models_outputs = [model.output for model in models]\n",
    "#   merged = layers.concatenate(models_outputs, axis=-1)\n",
    "#    merged_output = layers.Dense(output_days * output_features)(merged)\n",
    "#    final_output = layers.Reshape((output_days, output_features))(merged_output)\n",
    "#    ensemble_model = Model(inputs=models_inputs, outputs=final_output)\n",
    "#    ensemble_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "#\n",
    "#    return ensemble_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - loss: 0.0279 - mae: 0.1027 - val_loss: 5.5140e-04 - val_mae: 0.0176\n",
      "Epoch 2/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 5.2824e-04 - mae: 0.0169 - val_loss: 4.1390e-04 - val_mae: 0.0154\n",
      "Epoch 3/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 5.1239e-04 - mae: 0.0167 - val_loss: 4.6971e-04 - val_mae: 0.0162\n",
      "Epoch 4/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 4.3028e-04 - mae: 0.0155 - val_loss: 0.0011 - val_mae: 0.0263\n",
      "Epoch 5/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 7.6607e-04 - mae: 0.0210 - val_loss: 4.8477e-04 - val_mae: 0.0170\n",
      "Epoch 6/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 4.1826e-04 - mae: 0.0152 - val_loss: 3.3729e-04 - val_mae: 0.0135\n",
      "Epoch 7/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 3.9106e-04 - mae: 0.0147 - val_loss: 5.9622e-04 - val_mae: 0.0190\n",
      "Epoch 8/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 3.8556e-04 - mae: 0.0144 - val_loss: 3.2576e-04 - val_mae: 0.0133\n",
      "Epoch 9/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 3.8984e-04 - mae: 0.0145 - val_loss: 3.6576e-04 - val_mae: 0.0140\n",
      "Epoch 10/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 3.0942e-04 - mae: 0.0130 - val_loss: 0.0012 - val_mae: 0.0284\n",
      "Epoch 11/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 5.3522e-04 - mae: 0.0171 - val_loss: 3.7922e-04 - val_mae: 0.0143\n",
      "Epoch 12/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 4.1376e-04 - mae: 0.0154 - val_loss: 3.7486e-04 - val_mae: 0.0142\n",
      "Epoch 13/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 3.8007e-04 - mae: 0.0147 - val_loss: 3.5824e-04 - val_mae: 0.0140\n",
      "Epoch 14/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 3.6434e-04 - mae: 0.0141 - val_loss: 2.8506e-04 - val_mae: 0.0124\n",
      "Epoch 15/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 3.4567e-04 - mae: 0.0139 - val_loss: 6.9771e-04 - val_mae: 0.0211\n",
      "Epoch 16/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 3.9665e-04 - mae: 0.0147 - val_loss: 3.4478e-04 - val_mae: 0.0138\n",
      "Epoch 17/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 3.2562e-04 - mae: 0.0133 - val_loss: 2.6830e-04 - val_mae: 0.0118\n",
      "Epoch 18/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 3.8294e-04 - mae: 0.0148 - val_loss: 5.2331e-04 - val_mae: 0.0175\n",
      "Epoch 19/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 3.2525e-04 - mae: 0.0134 - val_loss: 7.1152e-04 - val_mae: 0.0207\n",
      "Epoch 20/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - loss: 4.0285e-04 - mae: 0.0149 - val_loss: 2.7196e-04 - val_mae: 0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#import pandas as pd\n",
    "#seed = 42\n",
    "#test_size = 0.8 # we need to consider for the ensemble model's training and ttesting data, since it cannot use the same training data\n",
    "#epochs = 20\n",
    "#batch_size = 8\n",
    "#shuffle=True\n",
    "#all_X_train = []\n",
    "#all_X_test = []\n",
    "#all_y_train = []\n",
    "#all_y_test = []\n",
    "#\n",
    "#y = pd.read_pickle(f\"/Users/hoyinchui/Downloads/y_a.pkl\")\n",
    "#for window in window_size:\n",
    "#    X = pd.read_pickle(f\"/Users/hoyinchui/Downloads/X_{window}days_a.pkl\")\n",
    "#    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=shuffle, random_state=seed)\n",
    "#    all_X_train.append(X_train)\n",
    "#    all_X_test.append(X_test)\n",
    "#    all_y_train.append(y_train)\n",
    "#    all_y_test.append(y_test)\n",
    "#\n",
    "##ensemble_model = ensemble_h_models(window_size, num_features, output_days, output_features)\n",
    "##history = ensemble_model.fit(all_X_train, all_y_train[0], epochs=epochs, batch_size=batch_size, validation_data=(all_X_test, all_y_test[0]), verbose=1)\n",
    "##ensemble_model.save(f\"ensemble_CNN_LSTM.h5\")\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
