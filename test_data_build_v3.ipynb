{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import yfinance as yf\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Data(ticker, start_date = \"2005-01-01\", end_date = \"2025-01-19\"):\n",
    "    try:\n",
    "        ticker_data = yf.Ticker(ticker)\n",
    "        historical_data = ticker_data.history(interval=\"1d\", start=start_date, end=end_date)\n",
    "        return historical_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for GDXJ...\n",
      "Fetching data for GDX...\n",
      "Fetching data for GLDM...\n"
     ]
    }
   ],
   "source": [
    "# base model data\n",
    "tickers = [\"GDXJ\", \"GDX\", \"GLDM\"]\n",
    "data_base = {}\n",
    "for ticker in tickers:\n",
    "    print(f\"Fetching data for {ticker}...\")\n",
    "    data = get_Data(ticker, start_date=\"2005-01-01\", end_date=\"2025-01-19\")\n",
    "    data = data.reset_index()\n",
    "    data_base[ticker] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target data(ensemble)\n",
    "target_stock = [\"GLD\"]\n",
    "data_ensemble = {}\n",
    "for ticker in target_stock:\n",
    "    target_stock_data = get_Data(ticker, start_date=\"2005-01-01\", end_date=\"2025-01-19\") \n",
    "    target_stock_data = target_stock_data.reset_index()\n",
    "    data_ensemble[ticker] = target_stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scale_features(df):\n",
    "    \"\"\"\n",
    "    Standardize the features in the DataFrame or Series.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.Series or pd.DataFrame): Input data to be standardized.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Standardized data with the same index and columns as the input.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Reshape the Series into a 2D array if it's 1D\n",
    "    if isinstance(df, pd.Series):\n",
    "        scaled_data = scaler.fit_transform(df.values.reshape(-1, 1))\n",
    "        return pd.DataFrame(scaled_data, columns=[df.name], index=df.index)\n",
    "    else:\n",
    "        scaled_data = scaler.fit_transform(df)\n",
    "        return pd.DataFrame(scaled_data, columns=df.columns, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scale_features(df: pd.DataFrame, feature_range=(0, 1)) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Min-Max scale the features in the DataFrame to a specific range.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the features to scale.\n",
    "        feature_range (tuple): Desired range of the scaled data (default: (0, 1)).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with scaled features.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler(feature_range=feature_range)\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "    return pd.DataFrame(scaled_data, columns=df.columns, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_scale_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust scale the features in the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the features to scale.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with scaled features.\n",
    "    \"\"\"\n",
    "    scaler = RobustScaler()\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "    return pd.DataFrame(scaled_data, columns=df.columns, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsi(df: pd.DataFrame, period: int = 14) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate the Relative Strength Index (RSI) using the 'Close' column.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing at least a 'Close' column.\n",
    "        period (int): Look-back period for the RSI (default: 14).\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: RSI values.\n",
    "    \"\"\"\n",
    "    close = df['Close']\n",
    "    delta = close.diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(window=period).mean()\n",
    "    loss = -delta.where(delta < 0, 0).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "\n",
    "def calculate_bollinger_band_width(df: pd.DataFrame, window: int = 20, num_std: float = 2) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate the Bollinger Bands Width using the 'Close' column.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing at least a 'Close' column.\n",
    "        window (int): Rolling window period (default: 20).\n",
    "        num_std (float): Number of standard deviations for the bands (default: 2).\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Bollinger Bands width.\n",
    "    \"\"\"\n",
    "    close = df['Close']\n",
    "    ma = close.rolling(window=window).mean()\n",
    "    std = close.rolling(window=window).std()\n",
    "    upper_band = ma + num_std * std\n",
    "    lower_band = ma - num_std * std\n",
    "    bb_width = (upper_band - lower_band) / ma\n",
    "    return bb_width\n",
    "\n",
    "\n",
    "def calculate_adx(df: pd.DataFrame, window: int = 14) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate the Average Directional Index (ADX) using the 'High', 'Low', and 'Close' columns\n",
    "    based on Wilder's smoothing method.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing 'High', 'Low', and 'Close' columns.\n",
    "        window (int): Look-back period (default: 14).\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: ADX values.\n",
    "    \"\"\"\n",
    "    high = df['High']\n",
    "    low = df['Low']\n",
    "    close = df['Close']\n",
    "    \n",
    "    # Calculate previous values\n",
    "    prev_close = close.shift(1)\n",
    "    prev_high = high.shift(1)\n",
    "    prev_low = low.shift(1)\n",
    "    \n",
    "    # True Range: max(high - low, abs(high - prev_close), abs(low - prev_close))\n",
    "    tr1 = high - low\n",
    "    tr2 = (high - prev_close).abs()\n",
    "    tr3 = (low - prev_close).abs()\n",
    "    true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    \n",
    "    # Directional Movements\n",
    "    up_move = high - prev_high\n",
    "    down_move = prev_low - low\n",
    "\n",
    "    plus_dm = up_move.where((up_move > down_move) & (up_move > 0), 0)\n",
    "    minus_dm = down_move.where((down_move > up_move) & (down_move > 0), 0)\n",
    "    \n",
    "    # Apply Wilder's smoothing using exponential moving average (alpha=1/window)\n",
    "    tr_smooth = true_range.ewm(alpha=1/window, min_periods=window, adjust=False).mean()\n",
    "    plus_dm_smooth = plus_dm.ewm(alpha=1/window, min_periods=window, adjust=False).mean()\n",
    "    minus_dm_smooth = minus_dm.ewm(alpha=1/window, min_periods=window, adjust=False).mean()\n",
    "\n",
    "    # Calculate Directional Indicators\n",
    "    plus_di = 100 * (plus_dm_smooth / tr_smooth)\n",
    "    minus_di = 100 * (minus_dm_smooth / tr_smooth)\n",
    "    \n",
    "    # DX: Directional Index\n",
    "    dx = 100 * ( (plus_di - minus_di).abs() / (plus_di + minus_di) )\n",
    "    \n",
    "    # ADX: Average Directional Index is the smoothed DX\n",
    "    adx = dx.ewm(alpha=1/window, min_periods=window, adjust=False).mean()\n",
    "    \n",
    "    return adx\n",
    "\n",
    "\n",
    "def calculate_volume_roc(df: pd.DataFrame, period: int = 20) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate the Volume Rate of Change (ROC) using the 'Volume' column.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing a 'Volume' column.\n",
    "        period (int): Look-back period (default: 20).\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Volume ROC values (in percent).\n",
    "    \"\"\"\n",
    "    volume = df['Volume']\n",
    "    volume_shifted = volume.shift(period)\n",
    "    roc = ((volume - volume_shifted) / volume_shifted) * 100\n",
    "    return roc\n",
    "\n",
    "\n",
    "def calculate_price_zscore(df: pd.DataFrame, window: int = 20) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate the rolling Z-score of the price using the 'Close' column.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing a 'Close' column.\n",
    "        window (int): Rolling window period (default: 20).\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Z-score values.\n",
    "    \"\"\"\n",
    "    close = df['Close']\n",
    "    rolling_mean = close.rolling(window=window).mean()\n",
    "    rolling_std = close.rolling(window=window).std()\n",
    "    zscore = (close - rolling_mean) / rolling_std\n",
    "    return zscore\n",
    "\n",
    "\n",
    "def calculate_skewness(df: pd.DataFrame, window: int = 20) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate the rolling skewness of the price using the 'Close' column.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing a 'Close' column.\n",
    "        window (int): Rolling window period (default: 20).\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Skewness values.\n",
    "    \"\"\"\n",
    "    close = df['Close']\n",
    "    return close.rolling(window=window).skew()\n",
    "\n",
    "\n",
    "def calculate_macd(df: pd.DataFrame, fast_period: int = 12, slow_period: int = 26, signal_period: int = 9) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the Moving Average Convergence Divergence (MACD) indicator using the 'Close' column.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing a 'Close' column.\n",
    "        fast_period (int): Period for the fast EMA (default: 12).\n",
    "        slow_period (int): Period for the slow EMA (default: 26).\n",
    "        signal_period (int): Period for the signal line EMA (default: 9).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with columns for MACD line, Signal line, and MACD Histogram.\n",
    "    \"\"\"\n",
    "    close = df['Close']\n",
    "    ema_fast = close.ewm(span=fast_period, adjust=False).mean()\n",
    "    ema_slow = close.ewm(span=slow_period, adjust=False).mean()\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    signal_line = macd_line.ewm(span=signal_period, adjust=False).mean()\n",
    "    macd_hist = macd_line - signal_line\n",
    "    return pd.DataFrame({\n",
    "        'MACD': macd_line,\n",
    "        'MACD_Signal': signal_line,\n",
    "        'MACD_Hist': macd_hist\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "def calculate_stochastic(df: pd.DataFrame, k_period: int = 14, d_period: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the Stochastic Oscillator using the 'High', 'Low', and 'Close' columns.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing 'High', 'Low', and 'Close' columns.\n",
    "        k_period (int): Look-back period for %K (default: 14).\n",
    "        d_period (int): Smoothing period for %D (default: 3).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with '%K' and '%D' columns.\n",
    "    \"\"\"\n",
    "    high = df['High']\n",
    "    low = df['Low']\n",
    "    close = df['Close']\n",
    "    lowest_low = low.rolling(window=k_period).min()\n",
    "    highest_high = high.rolling(window=k_period).max()\n",
    "    percent_k = 100 * (close - lowest_low) / (highest_high - lowest_low)\n",
    "    percent_d = percent_k.rolling(window=d_period).mean()\n",
    "    return pd.DataFrame({\n",
    "        'Stochastic_%K': percent_k,\n",
    "        'Stochastic_%D': percent_d\n",
    "    })\n",
    "\n",
    "\n",
    "def calculate_std(df: pd.DataFrame, window: int = 20) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate the rolling standard deviation using the 'Close' column.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing a 'Close' column.\n",
    "        window (int): Rolling window period (default: 20).\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Rolling standard deviation.\n",
    "    \"\"\"\n",
    "    close = df['Close']\n",
    "    return close.rolling(window=window).std()\n",
    "\n",
    "\n",
    "def calculate_fibonacci_retracement(df: pd.DataFrame,\n",
    "                                    fib_levels: list = [0.236, 0.382, 0.5, 0.618, 0.786]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate Fibonacci retracement levels based on the highest high and lowest low in the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing 'High' and 'Low' columns.\n",
    "        fib_levels (list): List of Fibonacci levels (default: [0.236, 0.382, 0.5, 0.618, 0.786]).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with constant Fibonacci levels for each row.\n",
    "    \"\"\"\n",
    "    swing_high = df['High'].max()\n",
    "    swing_low = df['Low'].min()\n",
    "    diff = swing_high - swing_low\n",
    "    levels = {f'Fib_{int(level*100)}': swing_high - diff * level for level in fib_levels}\n",
    "    fib_df = pd.DataFrame({key: [value] * len(df) for key, value in levels.items()}, index=df.index)\n",
    "    return standard_scale_features(fib_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a feature DataFrame by combining the original OHLCV (and any extra) columns\n",
    "    with various technical indicators computed from the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the necessary columns.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the original data and the calculated indicators.\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    features['Date'] = df['Date']\n",
    "\n",
    "    # Copy essential OHLCV data\n",
    "    for col in ['Open', 'High', 'Low', 'Close', 'Volume']:\n",
    "        if col in df.columns:\n",
    "            features[col] = df[col]\n",
    "    \n",
    "    # Copy additional columns if available\n",
    "    for col in ['Dividends', 'Stock Splits', 'Capital Gains']:\n",
    "        if col in df.columns:\n",
    "            features[col] = df[col]\n",
    "    period_ranges = [14, 20, 50, 200]\n",
    "    # Add technical indicators\n",
    "    for period in period_ranges:            \n",
    "        features[f'RSI_{period}'] = calculate_rsi(df, period=period)\n",
    "        features[f'RSI_{period}_n'] = standard_scale_features(features[[f'RSI_{period}']])\n",
    "        features[f'BB_Width_{period}'] = calculate_bollinger_band_width(df, window=period, num_std=2)\n",
    "        features[f'BB_Width_{period}_n'] = minmax_scale_features(features[[f'BB_Width_{period}']])\n",
    "        features[f'ADX_{period}'] = calculate_adx(df, window=period)\n",
    "        features[f'ADX_{period}_n'] = minmax_scale_features(features[[f'ADX_{period}']])\n",
    "        features[f'Volume_ROC_{period}'] = calculate_volume_roc(df, period=period)\n",
    "        features[f'Volume_ROC_{period}_n'] = standard_scale_features(features[[f'Volume_ROC_{period}']])\n",
    "        features[f'Price_Z_Score_{period}'] = calculate_price_zscore(df, window=period)\n",
    "        features[f'Price_Z_Score_{period}_n'] = standard_scale_features(features[[f'Price_Z_Score_{period}']])\n",
    "        features[f'Skewness_{period}'] = calculate_skewness(df, window=period)\n",
    "        features[f'Skewness_{period}_n'] = standard_scale_features(features[[f'Skewness_{period}']])\n",
    "        # Rolling standard deviation (e.g., of the 'Close' price)\n",
    "        features[f'Std_{period}'] = calculate_std(df, window=period)\n",
    "    \n",
    "    # Merge MACD and Stochastic indicators (returned as DataFrames)\n",
    "    macd_df = calculate_macd(df)\n",
    "    stochastic_df = calculate_stochastic(df)\n",
    "    features = features.join(macd_df)\n",
    "    features = features.join(stochastic_df)\n",
    "\n",
    "    \n",
    "    # Fibonacci retracement levels (constant across time, useful for overlays)\n",
    "    fib_df = calculate_fibonacci_retracement(df)\n",
    "    features = features.join(fib_df)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calendar_features (df: pd.DataFrame)-> pd.DataFrame:\n",
    "    df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
    "    df['is_trading_day'] = df['Date'].dt.dayofweek.apply(lambda x: 1 if x < 5 else 0)\n",
    "    df['is_trading_day_n'] = standard_scale_features(df[['is_trading_day']])\n",
    "    df['day_of_week'] = df['Date'].dt.dayofweek\n",
    "    df['day_of_week_n'] = standard_scale_features(df[['day_of_week']])\n",
    "    df['day_of_month'] = df['Date'].dt.day\n",
    "    df['day_of_month_n'] = standard_scale_features(df[['day_of_month']])\n",
    "    df['week_of_year'] = df['Date'].dt.isocalendar().week\n",
    "    df['week_of_year_n'] = standard_scale_features(df[['week_of_year']])\n",
    "    df['month'] = df['Date'].dt.month\n",
    "    df['month_n'] = standard_scale_features(df[['month']])\n",
    "    df['quarter'] = df['Date'].dt.quarter\n",
    "    df['quarter_n'] = standard_scale_features(df[['quarter']])\n",
    "\n",
    "    # Period End Indicators\n",
    "    df['is_month_end'] = df['Date'].dt.is_month_end.astype(int)\n",
    "    df['is_quarter_end'] = df['Date'].dt.is_quarter_end.astype(int)\n",
    "    df['is_year_end'] = df['Date'].dt.is_year_end.astype(int)\n",
    "    df['days_to_month_end'] = df['Date'].dt.days_in_month - df['Date'].dt.day\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def target_col_building(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     #calculating the differences between yesterday close price and today's low and high\n",
    "#     df['target_low'] = -(df['Close'].shift(-1) - df['Low'].shift(-1)) / df['Close'].shift(-1)\n",
    "#     df['target_high'] = -(df['Close'].shift(-1) - df['High'].shift(-1)) / df['Close'].shift(-1)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing features that have no info or no use or overlapped taht causing error\n",
    "def remove_duplicate_col(df, remove_features = [\"Date\",\"Dividends\", \"Stock Splits\", \"Capital Gains\", 'Open', 'High', 'Low', 'Close', 'Volume']):\n",
    "    df = df.drop(columns=remove_features)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_features(df):\n",
    "    for col in df.columns:\n",
    "        # finding percentage changes in orginal columns\n",
    "        if col != \"Date\" and not col.endswith('_n'):\n",
    "            df[f\"{col}_diff\"] = df[col].diff()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting columns that is end with\"_n\"\n",
    "def select_n_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    n_columns = [col for col in df.columns if col.endswith('_n') or col.endswith('_diff') ]\n",
    "    return df[n_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_1000(df):\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col] / 1000\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicator_building(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute technical indicators and merge them with the original dataset.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): A DataFrame containing the original data. The DataFrame\n",
    "                             must include the required columns (e.g., 'Open', 'High',\n",
    "                             'Low', 'Close', 'Volume') for computing the technical indicators.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame that contains both the original data columns and\n",
    "                      the additional technical indicator features.\n",
    "    \"\"\"\n",
    "    features_df = calculate_all_indicators(data)\n",
    "    #print(f\"Data shape: {features_df.shape}\")\n",
    "    features_df = calendar_features(features_df)\n",
    "    #print(f\"Data shape: {features_df.shape}\")\n",
    "    features_df = remove_duplicate_col(features_df)\n",
    "    #replace NaN values with 0, since when calculating the indicators, the first n values will be NaN\n",
    "    #features_df = features_df.fillna(0)\n",
    "    #dropping NaN values\n",
    "    #print(f\"Data shape: {data.shape}\")\n",
    "    data = data.join(features_df)\n",
    "    #print(f\"Data shape: {data.shape}\")\n",
    "    #print(f\"Data shape: {data.shape}\")\n",
    "    data = data.drop(columns=[\"Date\",'Dividends', 'Stock Splits','Capital Gains'])\n",
    "    data[[\"High_n\", \"Low_n\", \"Open_n\", \"Close_n\"]] = d_1000(data[[\"High\", \"Low\", \"Open\", \"Close\"]])\n",
    "    data[\"Volume_n\"] = standard_scale_features(data[\"Volume\"])\n",
    "    #print(data.head(10))\n",
    "    #print(f\"Data shape: {data.shape}\")\n",
    "    #building the diff features\n",
    "    data = diff_features(data)\n",
    "    data = data.dropna()\n",
    "    #data = select_n_columns(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####################################################################\n",
    "# #               individual-prediction-model design data             #\n",
    "# #####################################################################\n",
    "# def create_sliding_window_data(data, data_n, window_size, target_days):\n",
    "#     X, y = [], []\n",
    "\n",
    "#     # Loop through the data to create sliding windows\n",
    "#     for i in range(len(data) - window_size - target_days+1):\n",
    "#         X_window = data_n.iloc[i:i + window_size]  # Data from [i, i+window_size-1]\n",
    "#         #print(X_window[:10])\n",
    "#         y_target = data[[\"Low\",\"High\"]].iloc[i + window_size:i + window_size + target_days]  # Target: Low and High columns\n",
    "#         #print(y_target)\n",
    "#         X.append(X_window)\n",
    "#         y.append(y_target)\n",
    "\n",
    "#     # Convert to NumPy arrays\n",
    "#     X = np.array(X)\n",
    "#     y = np.array(y)\n",
    "\n",
    "#     return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the prediction to be the percentage change\n",
    "def create_sliding_window_data(data, data_n, window_size, target_days):\n",
    "    X, y = [], []\n",
    "\n",
    "    # Loop through the data to create sliding windows\n",
    "    for i in range(len(data) - window_size - target_days+1):\n",
    "        X_window = data_n.iloc[i:i + window_size]  # Data from [i, i+window_size-1]\n",
    "        #print(X_window[:10])\n",
    "        y_target = data[[\"Low\",\"High\"]].iloc[i + window_size:i + window_size + target_days]  # Target: Low and High columns\n",
    "        y_values_min_low = -((X_window[\"Close_n\"].iloc[-1]*1000)-y_target[\"Low\"].min())/(X_window[\"Close_n\"].iloc[-1]*1000)\n",
    "        y_values_max_high = -((X_window[\"Close_n\"].iloc[-1]*1000)-y_target[\"High\"].max())/(X_window[\"Close_n\"].iloc[-1]*1000)\n",
    "        #print(y_target)\n",
    "        X.append(X_window)\n",
    "        y.append([y_values_min_low, y_values_max_high])\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################################################\n",
    "# #               All-in-1-model design data             #\n",
    "# ########################################################\n",
    "# def create_sliding_window_data_a(i_from_window_size_loop, data, data_n, window_size, max_window_size, target_days):\n",
    "#     X, y = [], []\n",
    "\n",
    "#     # Loop through the data to create sliding windows\n",
    "#     for i in range(max_window_size, len(data) - target_days+1):\n",
    "#         if i_from_window_size_loop == 0:\n",
    "#             y_target = data[[\"Low\",\"High\"]].iloc[i :i + target_days]\n",
    "#             #print(y_target)\n",
    "#             y.append(y_target)\n",
    "#         X_window = data_n.iloc[i-window_size:i] # Data from [i, i+window_size-1]\n",
    "#         X.append(X_window)\n",
    "\n",
    "\n",
    "#     # Convert to NumPy arrays\n",
    "#     X = np.array(X)\n",
    "#     y = np.array(y)\n",
    "\n",
    "#     return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_window_data_a(i_from_window_size_loop, data, data_n, window_size, max_window_size, target_days):\n",
    "    X, y = [], []\n",
    "\n",
    "    # Loop through the data to create sliding windows\n",
    "    for i in range(max_window_size, len(data) - target_days+1):\n",
    "        if i_from_window_size_loop == 0:\n",
    "            y_target = data[[\"Low\",\"High\"]].iloc[i :i + target_days] \n",
    "            y_values_min_low = -(data[\"Close\"].iloc[i-1]-y_target[\"Low\"].min())/data[\"Close\"].iloc[i-1]\n",
    "            y_values_max_high = -(data[\"Close\"].iloc[i-1]-y_target[\"High\"].max())/data[\"Close\"].iloc[i-1]\n",
    "            #print(y_target)\n",
    "            y.append([y_values_min_low, y_values_max_high])\n",
    "        X_window = data_n.iloc[i-window_size:i] # Data from [i, i+window_size-1]\n",
    "        X.append(X_window)\n",
    "\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mz/2q2kcfjn7k55sw7cvzv6555w0000gn/T/ipykernel_98064/3857369038.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col] / 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for GDXJ: (3421, 141)\n",
      "Data shape for GDXJ with window size 30:\n",
      "X: (3385, 30, 88), y: (3385, 2)\n",
      "Data shape for GDXJ with window size 60:\n",
      "X: (3355, 60, 88), y: (3355, 2)\n",
      "Data shape for GDXJ with window size 90:\n",
      "X: (3325, 90, 88), y: (3325, 2)\n",
      "Data shape for GDXJ with window size 180:\n",
      "X: (3235, 180, 88), y: (3235, 2)\n",
      "Data shape for GDX: (4297, 141)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mz/2q2kcfjn7k55sw7cvzv6555w0000gn/T/ipykernel_98064/3857369038.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col] / 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for GDX with window size 30:\n",
      "X: (4261, 30, 88), y: (4261, 2)\n",
      "Data shape for GDX with window size 60:\n",
      "X: (4231, 60, 88), y: (4231, 2)\n",
      "Data shape for GDX with window size 90:\n",
      "X: (4201, 90, 88), y: (4201, 2)\n",
      "Data shape for GDX with window size 180:\n",
      "X: (4111, 180, 88), y: (4111, 2)\n",
      "Data shape for GLDM: (1252, 141)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mz/2q2kcfjn7k55sw7cvzv6555w0000gn/T/ipykernel_98064/3857369038.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col] / 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for GLDM with window size 30:\n",
      "X: (1216, 30, 88), y: (1216, 2)\n",
      "Data shape for GLDM with window size 60:\n",
      "X: (1186, 60, 88), y: (1186, 2)\n",
      "Data shape for GLDM with window size 90:\n",
      "X: (1156, 90, 88), y: (1156, 2)\n",
      "Data shape for GLDM with window size 180:\n",
      "X: (1066, 180, 88), y: (1066, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mz/2q2kcfjn7k55sw7cvzv6555w0000gn/T/ipykernel_98064/3857369038.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col] / 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for GLD with window size 30:\n",
      "X: (4459, 30, 88), y: (4459, 2)\n",
      "Data shape for GLD with window size 60:\n",
      "X: (4459, 60, 88), y: (0,)\n",
      "Data shape for GLD with window size 90:\n",
      "X: (4459, 90, 88), y: (0,)\n",
      "Data shape for GLD with window size 180:\n",
      "X: (4459, 180, 88), y: (0,)\n"
     ]
    }
   ],
   "source": [
    "# base model data indicators\n",
    "window_sizes = [30, 60, 90, 180] \n",
    "max_window_size = max(window_sizes)\n",
    "target_days = 7 \n",
    "base_X = {}\n",
    "base_y = {}\n",
    "for ticker in data_base:\n",
    "    data_base[ticker] = indicator_building(data_base[ticker])\n",
    "    data_n = select_n_columns(data_base[ticker])\n",
    "    #print(data_n.columns)\n",
    "    print(f\"Data shape for {ticker}: {data_base[ticker].shape}\")\n",
    "    #print(f\"Data shape for {ticker}: {featured_data.shape}\")\n",
    "    for window_size in window_sizes:\n",
    "        X, y = create_sliding_window_data(data_base[ticker],\n",
    "                                          data_n, \n",
    "                                          window_size=window_size, \n",
    "                                          target_days=target_days)\n",
    "        print(f\"Data shape for {ticker} with window size {window_size}:\")\n",
    "        print(f\"X: {X.shape}, y: {y.shape}\")\n",
    "        base_X[(ticker, window_size)] = X\n",
    "        base_y[(ticker, window_size)] = y\n",
    "    \n",
    "# ensemble model data indicators\n",
    "ensemble_X = {}\n",
    "ensemble_y = {}\n",
    "for ticker in data_ensemble:\n",
    "    data_ensemble = indicator_building(data_ensemble[ticker])\n",
    "    data_n = select_n_columns(data_ensemble)\n",
    "    for i in range(len(window_sizes)):\n",
    "        X, y = create_sliding_window_data_a(i, \n",
    "                                          data_ensemble, \n",
    "                                          data_n,\n",
    "                                          window_size = window_sizes[i], \n",
    "                                          max_window_size = max_window_size, \n",
    "                                          target_days = target_days)\n",
    "        print(f\"Data shape for {ticker} with window size {window_sizes[i]}:\")\n",
    "        print(f\"X: {X.shape}, y: {y.shape}\")\n",
    "        ensemble_X[(ticker, window_sizes[i])] = X\n",
    "        ensemble_y[(ticker, window_sizes[i])] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the data\n",
    "def normalize_data(data):\n",
    "    for key in data:\n",
    "        scaler = MinMaxScaler()\n",
    "        data[key] = scaler.fit_transform(data[key].reshape(-1, data[key].shape[-1])).reshape(data[key].shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_X_normalized = normalize_data(base_X)\n",
    "#ensemble_X_normalized = normalize_data(ensemble_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marging(base_X_normalized, base_y, window_sizes):\n",
    "    merged_X = {}\n",
    "    merged_y = {}\n",
    "\n",
    "    # Iterate over all window sizes\n",
    "    for window_size in window_sizes:\n",
    "        X_list = []  # List to collect all X data for the current window size\n",
    "        y_list = []  # List to collect all y data for the current window size\n",
    "        \n",
    "        # Iterate through the base_X_normalized dictionary to gather data for the current window size\n",
    "        for (ticker, size), data in base_X_normalized.items():\n",
    "            if size == window_size:\n",
    "                X = data  # Assuming the value of each key in the dictionary is a tuple (X, y)\n",
    "                X_list.append(X)\n",
    "        for (ticker, size), data in base_y.items():\n",
    "            if size == window_size:\n",
    "                y = data  # Assuming the value of each key in the dictionary is a tuple (X, y)\n",
    "                y_list.append(y)\n",
    "        \n",
    "        # Concatenate the X and y data for the current window size across all tickers\n",
    "        merged_X[window_size] = np.concatenate(X_list, axis=0)  # Concatenate X along the first axis (rows)\n",
    "        merged_y[window_size] = np.concatenate(y_list, axis=0) \n",
    "    return merged_X, merged_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_X, merged_y = marging(base_X_normalized, base_y, window_sizes)\n",
    "merged_X, merged_y = marging(base_X, base_y, window_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, filename, folder_path):\n",
    "    #create a folder if it does not exist\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    with open(f\"{folder_path}/{filename}\", \"wb\") as file:\n",
    "        pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for window_size in window_sizes:\n",
    "    save_data(merged_X[window_size], f\"X_{window_size}days_i.pkl\", \"data\")\n",
    "    save_data(merged_y[window_size], f\"y_{window_size}days_i.pkl\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for window_size in window_sizes:\n",
    "    save_data(ensemble_X[(target_stock[0], window_size)], f\"X_{window_size}days_a.pkl\", \"data\")\n",
    "    save_data(ensemble_y[(target_stock[0], window_sizes[0])], f\"y_a.pkl\", \"data\") #because we only build the y in the first itration, other is none"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
