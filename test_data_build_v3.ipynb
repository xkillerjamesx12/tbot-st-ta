{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "import yfinance as yf\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Data(ticker, start_date = \"2005-01-01\", end_date = \"2025-01-19\"):\n",
    "    try:\n",
    "        ticker_data = yf.Ticker(ticker)\n",
    "        historical_data = ticker_data.history(interval=\"1d\", start=start_date, end=end_date)\n",
    "        return historical_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for GDXJ...\n",
      "Fetching data for GDX...\n",
      "Fetching data for GLDM...\n"
     ]
    }
   ],
   "source": [
    "# base model data\n",
    "tickers = [\"GDXJ\", \"GDX\", \"GLDM\"]\n",
    "data_base = {}\n",
    "for ticker in tickers:\n",
    "    print(f\"Fetching data for {ticker}...\")\n",
    "    data = get_Data(ticker, start_date=\"2005-01-01\", end_date=\"2025-01-19\")\n",
    "    data = data.reset_index()\n",
    "    data_base[ticker] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target data(ensemble)\n",
    "target_stock = [\"GLD\"]\n",
    "data_ensemble = {}\n",
    "for ticker in target_stock:\n",
    "    target_stock_data = get_Data(ticker, start_date=\"2005-01-01\", end_date=\"2025-01-19\") \n",
    "    target_stock_data = target_stock_data.reset_index()\n",
    "    data_ensemble[ticker] = target_stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsi(df: pd.DataFrame, period: int = 14) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate the Relative Strength Index (RSI) using the 'Close' column.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing at least a 'Close' column.\n",
    "        period (int): Look-back period for the RSI (default: 14).\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: RSI values.\n",
    "    \"\"\"\n",
    "    close = df['Close']\n",
    "    delta = close.diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(window=period).mean()\n",
    "    loss = -delta.where(delta < 0, 0).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "\n",
    "def calculate_bollinger_band_width(df: pd.DataFrame, window: int = 20, num_std: float = 2) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate the Bollinger Bands Width using the 'Close' column.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing at least a 'Close' column.\n",
    "        window (int): Rolling window period (default: 20).\n",
    "        num_std (float): Number of standard deviations for the bands (default: 2).\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Bollinger Bands width.\n",
    "    \"\"\"\n",
    "    close = df['Close']\n",
    "    ma = close.rolling(window=window).mean()\n",
    "    std = close.rolling(window=window).std()\n",
    "    upper_band = ma + num_std * std\n",
    "    lower_band = ma - num_std * std\n",
    "    bb_width = (upper_band - lower_band) / ma\n",
    "    return bb_width\n",
    "\n",
    "\n",
    "def calculate_adx(df: pd.DataFrame, window: int = 14) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate the Average Directional Index (ADX) using the 'High', 'Low', and 'Close' columns\n",
    "    based on Wilder's smoothing method.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing 'High', 'Low', and 'Close' columns.\n",
    "        window (int): Look-back period (default: 14).\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: ADX values.\n",
    "    \"\"\"\n",
    "    high = df['High']\n",
    "    low = df['Low']\n",
    "    close = df['Close']\n",
    "    \n",
    "    # Calculate previous values\n",
    "    prev_close = close.shift(1)\n",
    "    prev_high = high.shift(1)\n",
    "    prev_low = low.shift(1)\n",
    "    \n",
    "    # True Range: max(high - low, abs(high - prev_close), abs(low - prev_close))\n",
    "    tr1 = high - low\n",
    "    tr2 = (high - prev_close).abs()\n",
    "    tr3 = (low - prev_close).abs()\n",
    "    true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    \n",
    "    # Directional Movements\n",
    "    up_move = high - prev_high\n",
    "    down_move = prev_low - low\n",
    "\n",
    "    plus_dm = up_move.where((up_move > down_move) & (up_move > 0), 0)\n",
    "    minus_dm = down_move.where((down_move > up_move) & (down_move > 0), 0)\n",
    "    \n",
    "    # Apply Wilder's smoothing using exponential moving average (alpha=1/window)\n",
    "    tr_smooth = true_range.ewm(alpha=1/window, min_periods=window, adjust=False).mean()\n",
    "    plus_dm_smooth = plus_dm.ewm(alpha=1/window, min_periods=window, adjust=False).mean()\n",
    "    minus_dm_smooth = minus_dm.ewm(alpha=1/window, min_periods=window, adjust=False).mean()\n",
    "\n",
    "    # Calculate Directional Indicators\n",
    "    plus_di = 100 * (plus_dm_smooth / tr_smooth)\n",
    "    minus_di = 100 * (minus_dm_smooth / tr_smooth)\n",
    "    \n",
    "    # DX: Directional Index\n",
    "    dx = 100 * ( (plus_di - minus_di).abs() / (plus_di + minus_di) )\n",
    "    \n",
    "    # ADX: Average Directional Index is the smoothed DX\n",
    "    adx = dx.ewm(alpha=1/window, min_periods=window, adjust=False).mean()\n",
    "    \n",
    "    return adx\n",
    "\n",
    "\n",
    "def calculate_volume_roc(df: pd.DataFrame, period: int = 20) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate the Volume Rate of Change (ROC) using the 'Volume' column.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing a 'Volume' column.\n",
    "        period (int): Look-back period (default: 20).\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Volume ROC values (in percent).\n",
    "    \"\"\"\n",
    "    volume = df['Volume']\n",
    "    volume_shifted = volume.shift(period)\n",
    "    roc = ((volume - volume_shifted) / volume_shifted) * 100\n",
    "    return roc\n",
    "\n",
    "\n",
    "def calculate_price_zscore(df: pd.DataFrame, window: int = 20) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate the rolling Z-score of the price using the 'Close' column.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing a 'Close' column.\n",
    "        window (int): Rolling window period (default: 20).\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Z-score values.\n",
    "    \"\"\"\n",
    "    close = df['Close']\n",
    "    rolling_mean = close.rolling(window=window).mean()\n",
    "    rolling_std = close.rolling(window=window).std()\n",
    "    zscore = (close - rolling_mean) / rolling_std\n",
    "    return zscore\n",
    "\n",
    "\n",
    "def calculate_skewness(df: pd.DataFrame, window: int = 20) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate the rolling skewness of the price using the 'Close' column.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing a 'Close' column.\n",
    "        window (int): Rolling window period (default: 20).\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Skewness values.\n",
    "    \"\"\"\n",
    "    close = df['Close']\n",
    "    return close.rolling(window=window).skew()\n",
    "\n",
    "\n",
    "def calculate_macd(df: pd.DataFrame, fast_period: int = 12, slow_period: int = 26, signal_period: int = 9) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the Moving Average Convergence Divergence (MACD) indicator using the 'Close' column.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing a 'Close' column.\n",
    "        fast_period (int): Period for the fast EMA (default: 12).\n",
    "        slow_period (int): Period for the slow EMA (default: 26).\n",
    "        signal_period (int): Period for the signal line EMA (default: 9).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with columns for MACD line, Signal line, and MACD Histogram.\n",
    "    \"\"\"\n",
    "    close = df['Close']\n",
    "    ema_fast = close.ewm(span=fast_period, adjust=False).mean()\n",
    "    ema_slow = close.ewm(span=slow_period, adjust=False).mean()\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    signal_line = macd_line.ewm(span=signal_period, adjust=False).mean()\n",
    "    macd_hist = macd_line - signal_line\n",
    "    return pd.DataFrame({\n",
    "        'MACD': macd_line,\n",
    "        'MACD_Signal': signal_line,\n",
    "        'MACD_Hist': macd_hist\n",
    "    })\n",
    "\n",
    "\n",
    "def calculate_stochastic(df: pd.DataFrame, k_period: int = 14, d_period: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the Stochastic Oscillator using the 'High', 'Low', and 'Close' columns.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing 'High', 'Low', and 'Close' columns.\n",
    "        k_period (int): Look-back period for %K (default: 14).\n",
    "        d_period (int): Smoothing period for %D (default: 3).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with '%K' and '%D' columns.\n",
    "    \"\"\"\n",
    "    high = df['High']\n",
    "    low = df['Low']\n",
    "    close = df['Close']\n",
    "    lowest_low = low.rolling(window=k_period).min()\n",
    "    highest_high = high.rolling(window=k_period).max()\n",
    "    percent_k = 100 * (close - lowest_low) / (highest_high - lowest_low)\n",
    "    percent_d = percent_k.rolling(window=d_period).mean()\n",
    "    return pd.DataFrame({\n",
    "        'Stochastic_%K': percent_k,\n",
    "        'Stochastic_%D': percent_d\n",
    "    })\n",
    "\n",
    "\n",
    "def calculate_std(df: pd.DataFrame, window: int = 20) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate the rolling standard deviation using the 'Close' column.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing a 'Close' column.\n",
    "        window (int): Rolling window period (default: 20).\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Rolling standard deviation.\n",
    "    \"\"\"\n",
    "    close = df['Close']\n",
    "    return close.rolling(window=window).std()\n",
    "\n",
    "\n",
    "def calculate_fibonacci_retracement(df: pd.DataFrame,\n",
    "                                    fib_levels: list = [0.236, 0.382, 0.5, 0.618, 0.786]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate Fibonacci retracement levels based on the highest high and lowest low in the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing 'High' and 'Low' columns.\n",
    "        fib_levels (list): List of Fibonacci levels (default: [0.236, 0.382, 0.5, 0.618, 0.786]).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with constant Fibonacci levels for each row.\n",
    "    \"\"\"\n",
    "    swing_high = df['High'].max()\n",
    "    swing_low = df['Low'].min()\n",
    "    diff = swing_high - swing_low\n",
    "    levels = {f'Fib_{int(level*100)}': swing_high - diff * level for level in fib_levels}\n",
    "    fib_df = pd.DataFrame({key: [value] * len(df) for key, value in levels.items()}, index=df.index)\n",
    "    return fib_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a feature DataFrame by combining the original OHLCV (and any extra) columns\n",
    "    with various technical indicators computed from the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the necessary columns.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the original data and the calculated indicators.\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    features['Date'] = df['Date']\n",
    "\n",
    "    # Copy essential OHLCV data\n",
    "    for col in ['Open', 'High', 'Low', 'Close', 'Volume']:\n",
    "        if col in df.columns:\n",
    "            features[col] = df[col]\n",
    "    \n",
    "    # Copy additional columns if available\n",
    "    for col in ['Dividends', 'Stock Splits', 'Capital Gains']:\n",
    "        if col in df.columns:\n",
    "            features[col] = df[col]\n",
    "    period_ranges = [14, 20, 50, 200]\n",
    "    # Add technical indicators\n",
    "    for period in period_ranges:            \n",
    "        features[f'RSI_{period}'] = calculate_rsi(df, period=period)\n",
    "        features[f'BB_Width_{period}'] = calculate_bollinger_band_width(df, window=period, num_std=2)\n",
    "        features[f'ADX_{period}'] = calculate_adx(df, window=period)\n",
    "        features[f'Volume_ROC_{period}'] = calculate_volume_roc(df, period=period)\n",
    "        features[f'Price_Z_Score_{period}'] = calculate_price_zscore(df, window=period)\n",
    "        features[f'Skewness_{period}'] = calculate_skewness(df, window=period)\n",
    "        # Rolling standard deviation (e.g., of the 'Close' price)\n",
    "        features[f'Std_{period}'] = calculate_std(df, window=period)\n",
    "    \n",
    "    # Merge MACD and Stochastic indicators (returned as DataFrames)\n",
    "    macd_df = calculate_macd(df)\n",
    "    stochastic_df = calculate_stochastic(df)\n",
    "    features = features.join(macd_df)\n",
    "    features = features.join(stochastic_df)\n",
    "\n",
    "    \n",
    "    # Fibonacci retracement levels (constant across time, useful for overlays)\n",
    "    fib_df = calculate_fibonacci_retracement(df)\n",
    "    features = features.join(fib_df)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calendar_features (df: pd.DataFrame)-> pd.DataFrame:\n",
    "    df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
    "    df['is_trading_day'] = df['Date'].dt.dayofweek.apply(lambda x: 1 if x < 5 else 0)\n",
    "    df['day_of_week'] = df['Date'].dt.dayofweek\n",
    "    df['day_of_month'] = df['Date'].dt.day\n",
    "    df['week_of_year'] = df['Date'].dt.isocalendar().week\n",
    "    df['month'] = df['Date'].dt.month\n",
    "    df['quarter'] = df['Date'].dt.quarter\n",
    "\n",
    "    # Period End Indicators\n",
    "    df['is_month_end'] = df['Date'].dt.is_month_end.astype(int)\n",
    "    df['is_quarter_end'] = df['Date'].dt.is_quarter_end.astype(int)\n",
    "    df['is_year_end'] = df['Date'].dt.is_year_end.astype(int)\n",
    "    df['days_to_month_end'] = df['Date'].dt.days_in_month - df['Date'].dt.day\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing features that have no info or no use or overlapped taht causing error\n",
    "def feature_selection(df, remove_features = [\"Date\",\"Dividends\", \"Stock Splits\", \"Capital Gains\", 'Open', 'High', 'Low', 'Close', 'Volume']):\n",
    "    df = df.drop(columns=remove_features)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicator_building(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute technical indicators and merge them with the original dataset.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): A DataFrame containing the original data. The DataFrame\n",
    "                             must include the required columns (e.g., 'Open', 'High',\n",
    "                             'Low', 'Close', 'Volume') for computing the technical indicators.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame that contains both the original data columns and\n",
    "                      the additional technical indicator features.\n",
    "    \"\"\"\n",
    "    features_df = calculate_all_indicators(data)\n",
    "    #print(f\"Data shape: {features_df.shape}\")\n",
    "    features_df = calendar_features(features_df)\n",
    "    #print(f\"Data shape: {features_df.shape}\")\n",
    "    features_df = feature_selection(features_df)\n",
    "    #replace NaN values with 0, since when calculating the indicators, the first n values will be NaN\n",
    "    #features_df = features_df.fillna(0)\n",
    "    #dropping NaN values\n",
    "    #print(f\"Data shape: {data.shape}\")\n",
    "    data = data.join(features_df)\n",
    "    #print(f\"Data shape: {data.shape}\")\n",
    "    data = data.dropna()\n",
    "    #print(f\"Data shape: {data.shape}\")\n",
    "    data = data.drop(columns=[\"Date\"])\n",
    "    #print(f\"Data shape: {data.shape}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "#               individual-prediction-model design data             #\n",
    "#####################################################################\n",
    "def create_sliding_window_data(data, window_size, target_days):\n",
    "    X, y = [], []\n",
    "\n",
    "    # Loop through the data to create sliding windows\n",
    "    for i in range(len(data) - window_size - target_days+1):\n",
    "        X_window = data.iloc[i:i + window_size].drop(columns=data.columns[2:4])  # Data from [i, i+window_size-1]\n",
    "        #print(X_window[:10])\n",
    "        y_target = data.iloc[i + window_size:i + window_size + target_days, 2:4]  # Target: Low and High columns\n",
    "        #print(y_target)\n",
    "        X.append(X_window)\n",
    "        y.append(y_target)\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "#               All-in-1-model design data             #\n",
    "########################################################\n",
    "def create_sliding_window_data_a(i_from_window_size_loop, data, window_size, max_window_size, target_days):\n",
    "    X, y = [], []\n",
    "\n",
    "    # Loop through the data to create sliding windows\n",
    "    for i in range(max_window_size, len(data) - target_days+1):\n",
    "        if i_from_window_size_loop == 0:\n",
    "            y_target = data.iloc[i :i + target_days, 2:4] \n",
    "            #print(y_target)\n",
    "            y.append(y_target)\n",
    "        X_window = data.iloc[i-window_size:i].drop(columns=data.columns[2:4]) # Data from [i, i+window_size-1]\n",
    "        X.append(X_window)\n",
    "\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for GDXJ: (3422, 56)\n",
      "Data shape for GDXJ with window size 14:\n",
      "X: (3402, 14, 54), y: (3402, 7, 2)\n",
      "Data shape for GDXJ with window size 30:\n",
      "X: (3386, 30, 54), y: (3386, 7, 2)\n",
      "Data shape for GDXJ with window size 60:\n",
      "X: (3356, 60, 54), y: (3356, 7, 2)\n",
      "Data shape for GDXJ with window size 90:\n",
      "X: (3326, 90, 54), y: (3326, 7, 2)\n",
      "Data shape for GDXJ with window size 180:\n",
      "X: (3236, 180, 54), y: (3236, 7, 2)\n",
      "Data shape for GDX: (4298, 56)\n",
      "Data shape for GDX with window size 14:\n",
      "X: (4278, 14, 54), y: (4278, 7, 2)\n",
      "Data shape for GDX with window size 30:\n",
      "X: (4262, 30, 54), y: (4262, 7, 2)\n",
      "Data shape for GDX with window size 60:\n",
      "X: (4232, 60, 54), y: (4232, 7, 2)\n",
      "Data shape for GDX with window size 90:\n",
      "X: (4202, 90, 54), y: (4202, 7, 2)\n",
      "Data shape for GDX with window size 180:\n",
      "X: (4112, 180, 54), y: (4112, 7, 2)\n",
      "Data shape for GLDM: (1253, 56)\n",
      "Data shape for GLDM with window size 14:\n",
      "X: (1233, 14, 54), y: (1233, 7, 2)\n",
      "Data shape for GLDM with window size 30:\n",
      "X: (1217, 30, 54), y: (1217, 7, 2)\n",
      "Data shape for GLDM with window size 60:\n",
      "X: (1187, 60, 54), y: (1187, 7, 2)\n",
      "Data shape for GLDM with window size 90:\n",
      "X: (1157, 90, 54), y: (1157, 7, 2)\n",
      "Data shape for GLDM with window size 180:\n",
      "X: (1067, 180, 54), y: (1067, 7, 2)\n",
      "Data shape for GLD with window size 14:\n",
      "X: (4460, 14, 54), y: (4460, 7, 2)\n",
      "Data shape for GLD with window size 30:\n",
      "X: (4460, 30, 54), y: (0,)\n",
      "Data shape for GLD with window size 60:\n",
      "X: (4460, 60, 54), y: (0,)\n",
      "Data shape for GLD with window size 90:\n",
      "X: (4460, 90, 54), y: (0,)\n",
      "Data shape for GLD with window size 180:\n",
      "X: (4460, 180, 54), y: (0,)\n"
     ]
    }
   ],
   "source": [
    "# base model data indicators\n",
    "window_sizes = [14, 30, 60, 90, 180] \n",
    "max_window_size = max(window_sizes)\n",
    "target_days = 7 \n",
    "base_X = {}\n",
    "base_y = {}\n",
    "for ticker in data_base:\n",
    "    data_base[ticker] = indicator_building(data_base[ticker])\n",
    "    print(f\"Data shape for {ticker}: {data_base[ticker].shape}\")\n",
    "    #print(f\"Data shape for {ticker}: {featured_data.shape}\")\n",
    "    for window_size in window_sizes:\n",
    "        X, y = create_sliding_window_data(data_base[ticker], \n",
    "                                          window_size=window_size, \n",
    "                                          target_days=target_days)\n",
    "        print(f\"Data shape for {ticker} with window size {window_size}:\")\n",
    "        print(f\"X: {X.shape}, y: {y.shape}\")\n",
    "        base_X[(ticker, window_size)] = X\n",
    "        base_y[(ticker, window_size)] = y\n",
    "    \n",
    "# ensemble model data indicators\n",
    "ensemble_X = {}\n",
    "ensemble_y = {}\n",
    "for ticker in data_ensemble:\n",
    "    data_ensemble = indicator_building(data_ensemble[ticker])\n",
    "    for i in range(len(window_sizes)):\n",
    "        X, y = create_sliding_window_data_a(i, \n",
    "                                          data_ensemble, \n",
    "                                          window_size = window_sizes[i], \n",
    "                                          max_window_size = max_window_size, \n",
    "                                          target_days = target_days)\n",
    "        print(f\"Data shape for {ticker} with window size {window_sizes[i]}:\")\n",
    "        print(f\"X: {X.shape}, y: {y.shape}\")\n",
    "        ensemble_X[(ticker, window_sizes[i])] = X\n",
    "        ensemble_y[(ticker, window_sizes[i])] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the data\n",
    "def normalize_data(data):\n",
    "    for key in data:\n",
    "        scaler = MinMaxScaler()\n",
    "        data[key] = scaler.fit_transform(data[key].reshape(-1, data[key].shape[-1])).reshape(data[key].shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_X_normalized = normalize_data(base_X)\n",
    "ensemble_X_normalized = normalize_data(ensemble_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marging(base_X_normalized, base_y, window_sizes):\n",
    "    merged_X = {}\n",
    "    merged_y = {}\n",
    "\n",
    "    # Iterate over all window sizes\n",
    "    for window_size in window_sizes:\n",
    "        X_list = []  # List to collect all X data for the current window size\n",
    "        y_list = []  # List to collect all y data for the current window size\n",
    "        \n",
    "        # Iterate through the base_X_normalized dictionary to gather data for the current window size\n",
    "        for (ticker, size), data in base_X_normalized.items():\n",
    "            if size == window_size:\n",
    "                X = data  # Assuming the value of each key in the dictionary is a tuple (X, y)\n",
    "                X_list.append(X)\n",
    "        for (ticker, size), data in base_y.items():\n",
    "            if size == window_size:\n",
    "                y = data  # Assuming the value of each key in the dictionary is a tuple (X, y)\n",
    "                y_list.append(y)\n",
    "        \n",
    "        # Concatenate the X and y data for the current window size across all tickers\n",
    "        merged_X[window_size] = np.concatenate(X_list, axis=0)  # Concatenate X along the first axis (rows)\n",
    "        merged_y[window_size] = np.concatenate(y_list, axis=0) \n",
    "    return merged_X, merged_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_X, merged_y = marging(base_X_normalized, base_y, window_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, filename, folder_path):\n",
    "    #create a folder if it does not exist\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    with open(f\"{folder_path}/{filename}\", \"wb\") as file:\n",
    "        pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for window_size in window_sizes:\n",
    "    save_data(merged_X[window_size], f\"X_{window_size}days_i.pkl\", \"data\")\n",
    "    save_data(merged_y[window_size], f\"y_{window_size}days_i.pkl\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for window_size in window_sizes:\n",
    "    save_data(ensemble_X_normalized[(target_stock[0], window_size)], f\"X_{window_size}days_a.pkl\", \"data\")\n",
    "    save_data(ensemble_y[(target_stock[0], window_sizes[0])], f\"y_a.pkl\", \"data\") #because we only build the y in the first itration, other is none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference (data):\n",
    "    #the data would be a df from the raw data\n",
    "    #we will difference the data with the shift function\n",
    "    #return the differenced data\n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
